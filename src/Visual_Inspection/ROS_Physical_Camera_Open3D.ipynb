{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b930f95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import rospy\n",
    "\n",
    "import pyrealsense2\n",
    "from realsense_depth import *\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import open3d as o3d\n",
    "from scipy import spatial\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import math\n",
    "import tf2_ros, tf\n",
    "import geometry_msgs.msg\n",
    "import PyKDL\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import moveit_commander\n",
    "import moveit_msgs.msg\n",
    "from moveit_commander.conversions import pose_to_list\n",
    "\n",
    "import subprocess\n",
    "from ast import literal_eval\n",
    "import copy\n",
    "from PIL import Image as PIL_img\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "image = None\n",
    "\n",
    "\n",
    "rospy.init_node('my_pic', anonymous=True)\n",
    "\n",
    "moveit_commander.roscpp_initialize(sys.argv)\n",
    "robot = moveit_commander.RobotCommander()\n",
    "scene = moveit_commander.PlanningSceneInterface()\n",
    "group_name = \"panda_arm\"\n",
    "move_group = moveit_commander.MoveGroupCommander(group_name) #we'll pass it on while calling functions\n",
    "\n",
    "\n",
    "tfbuffer = tf2_ros.Buffer()\n",
    "listener = tf2_ros.TransformListener(tfbuffer)\n",
    "br = tf2_ros.TransformBroadcaster()\n",
    "t = geometry_msgs.msg.TransformStamped()\n",
    "\n",
    "static_br = tf2_ros.StaticTransformBroadcaster()\n",
    "static_t = geometry_msgs.msg.TransformStamped()\n",
    "\n",
    "bridge = CvBridge()\n",
    "loop_rate = rospy.Rate(0.5) # Node cycle rate (in Hz).\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Initialize Camera Intel Realsense\n",
    "#dc = DepthCamera()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07fa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a31f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!!\n"
     ]
    }
   ],
   "source": [
    "def grab_frame():\n",
    "   \n",
    "    frame_color=rospy.wait_for_message('/camera/color/image_raw', Image, timeout=None) #wait_for_message(topic, topic_type, timeout=None): \n",
    "    cv_image_color = bridge.imgmsg_to_cv2(frame_color, desired_encoding='rgb8')\n",
    "    #frame_depth = rospy.wait_for_message('/camera/aligned_depth_to_color/image_raw', Image, timeout=None) #wait_for_message(topic, topic_type, timeout=None): \n",
    "    frame_depth = rospy.wait_for_message('/camera/depth/image_rect_raw', Image, timeout=None) #wait_for_message(topic, topic_type, timeout=None): \n",
    "    cv_image_depth = bridge.imgmsg_to_cv2(frame_depth)\n",
    "    \n",
    "    return cv_image_color, cv_image_depth\n",
    "\n",
    "def Generate_PointCloud(color_frame,depth_frame,from_depth=False):\n",
    "\n",
    "    #intrinsic.intrinsic_matrix =  [[602.71783447, 0.0, 313.06835938], [0.0, 601.61364746, 230.37461853], [0.0, 0.0, 1.0]]\n",
    "    '''\n",
    "     Intrinsic of \"Color\" / 640x480 / {YUYV/RGB8/BGR8/RGBA8/BGRA8/Y8/Y16}\n",
    "      Width:      \t640\n",
    "      Height:     \t480\n",
    "      PPX:        \t313.068359375\n",
    "      PPY:        \t230.374618530273\n",
    "      Fx:         \t602.717834472656\n",
    "      Fy:         \t601.613647460938\n",
    "      Distortion: \tInverse Brown Conrady\n",
    "      Coeffs:     \t0  \t0  \t0  \t0  \t0  \n",
    "      FOV (deg):  \t55.93 x 43.49\n",
    "    '''\n",
    "    \n",
    "    #From camera  Intrinsic of \"Depth\" / 640x480 / {Z16} \n",
    "    #Distortion: Brown Conrady  Coeffs:  0  0  0  0  0   FOV (deg): 79.93 x 64.3\n",
    "    w = 640\n",
    "    h = 480\n",
    "    fx = 381.838836669922\n",
    "    fy = 381.838836669922\n",
    "    cx = 320.060424804688\n",
    "    cy = 237.036407470703\n",
    "    \n",
    "    #Depth frame ROS camera\n",
    "    #w = 640\n",
    "    #h = 480\n",
    "    #fx = 347.99755859375\n",
    "    #fy = 347.99755859375\n",
    "    #cx = 320.0\n",
    "    #cy = 240.0    \n",
    "    \n",
    "    cam = o3d.camera.PinholeCameraParameters()\n",
    "    cam.intrinsic = o3d.camera.PinholeCameraIntrinsic(w, h, fx,fy, cx, cy)\n",
    "    #intrinsic.intrinsic_matrix = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]  #alternative method\n",
    "    '''   \n",
    "    extr_T = [0.0150703,0.000300758,9.12806e-06] #Depth to RGB\n",
    "    extr_R = [[0.999992,0.00279048,0.00277953],\n",
    "              [-0.00280054,0.99999,0.00362144],\n",
    "              [-0.0027694,-0.00362919,0.99999]]  '''\n",
    "\n",
    "    \n",
    "    extr_T = [-0.0150693,-0.000342775,-5.21054e-05]   #RGB to Depth\n",
    "    extr_R = [[0.999992,-0.00280054,-0.0027694],\n",
    "              [0.00279048,0.99999,-0.00362919],\n",
    "              [0.00277953,0.00362144,0.99999]]  \n",
    "    \n",
    "    cam.extrinsic = np.array([[extr_R[0][0], extr_R[0][1], extr_R[0][2], extr_T[0]], \n",
    "                                 [extr_R[1][0], extr_R[1][1], extr_R[1][2], extr_T[1]], \n",
    "                                 [extr_R[2][0], extr_R[2][1], extr_R[2][2], extr_T[2]], \n",
    "                                 [0.          , 0.          , 0.          , 1.      ]])\n",
    "\n",
    "    color_raw = o3d.geometry.Image(np.asarray(color_frame))\n",
    "    depth_raw = o3d.geometry.Image(np.asarray(depth_frame.astype(np.uint16)))\n",
    "    \n",
    "    if from_depth==False:\n",
    "        rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(color_raw, depth_raw, convert_rgb_to_intensity=False)\n",
    "        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, cam.intrinsic, cam.extrinsic)\n",
    "        #pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, cam.intrinsic)\n",
    "    else:\n",
    "        pcd =  o3d.geometry.PointCloud.create_from_depth_image(depth_raw, cam.intrinsic, cam.extrinsic, \n",
    "                                                               depth_scale=1000.0, depth_trunc=1000.0, \n",
    "                                                               stride=1, project_valid_depth_only=True)\n",
    "    \n",
    "    pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]) # Flip it, otherwise the pointcloud will be upside down.\n",
    "    return pcd\n",
    "\n",
    "\n",
    "def fetch_transform(tfbuffer,frame1,frame2,quat=0):\n",
    "    flag = 0\n",
    "    while flag==0:\n",
    "        try:\n",
    "            trans = tfbuffer.lookup_transform(frame1, frame2, rospy.Time(),rospy.Duration(8.0))\n",
    "            #print (trans)\n",
    "            trans = trans.transform  #save translation and rotation\n",
    "            #rot = PyKDL.Rotation.Quaternion(* [ trans.rotation.x,trans.rotation.y,trans.rotation.z,trans.rotation.w] )\n",
    "            #ypr = [ i  / np.pi * 180 for i in rot.GetEulerZYX() ]\n",
    "            #print(ypr[2],ypr[1],ypr[0])\n",
    "            \n",
    "            rot = R.from_quat([ trans.rotation.x,trans.rotation.y,trans.rotation.z,trans.rotation.w]) #creates rotation matrix\n",
    "            rpy = rot.as_euler('XYZ',degrees=True) #extrinsic\n",
    "            break\n",
    "        except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException) as e:\n",
    "            #print (\"Fail\", e)\n",
    "            continue\n",
    "    if quat==0:\n",
    "        return trans.translation.x, trans.translation.y, trans.translation.z, rpy[0],rpy[1],rpy[2] #ypr[2], ypr[1], ypr[0]\n",
    "    elif quat==1:\n",
    "        return trans.translation.x, trans.translation.y, trans.translation.z, trans.rotation.x, trans.rotation.y, trans.rotation.z, trans.rotation.w\n",
    "    \n",
    "\n",
    "\n",
    "def Axis_angle_to_Quat(vector,angle):\n",
    "    x=vector[0]\n",
    "    y=vector[1]\n",
    "    z=vector[2]\n",
    "    qx = x * np.sin(angle/2)\n",
    "    qy = y * np.sin(angle/2)\n",
    "    qz = z * np.sin(angle/2)\n",
    "    qw = np.cos(angle/2)\n",
    "    \n",
    "    return qx,qy,qz,qw\n",
    "\n",
    "def Axis_angle_to_Euler(vector,angle):\n",
    "    x=vector[0]\n",
    "    y=vector[1]\n",
    "    z=vector[2]\n",
    "    s=np.sin(angle)\n",
    "    c=np.cos(angle)\n",
    "    t=1-c\n",
    "    \n",
    "    if ((x*y*t + z*s) > 0.998):  #north pole singularity detected\n",
    "        \n",
    "        heading = 2 * np.arctan2(x*np.sin(angle/2), np.cos(angle/2))\n",
    "        attitude = np.pi/2\n",
    "        bank = 0\n",
    "        return heading, attitude, bank\n",
    "    \n",
    "    elif ((x*y*t + z*s) < -0.998):\n",
    "        \n",
    "        heading = -2 * np.arctan2(x*np.sin(angle/2), np.cos(angle/2))\n",
    "        attitude = -np.pi/2\n",
    "        bank = 0\n",
    "        return heading, attitude, bank\n",
    "    \n",
    "    heading = np.arctan2(y * s- x * z * t , 1 - (y*y+ z*z ) * t)\n",
    "    attitude = np.arcsin(x * y * t + z * s)\n",
    "    bank = np.arctan2(x * s - y * z * t , 1 - (x*x + z*z) * t)\n",
    "    \n",
    "    return bank, heading, attitude\n",
    "\n",
    "\n",
    "def getRotation(v1):\n",
    "    if np.all(v1==[0., 0., 1.]): v1 = [0, 0.000001, 0.999999]\n",
    "    vec_x = [1.,0.,0.] \n",
    "    vec_y = [0.,1.,0.]\n",
    "    vec_z = [0.,0.,1.] #>>>>>>>>>>>>>>>>>>>>> change these to -1 to flip again\n",
    "    \n",
    "    vec1 = v1 / np.linalg.norm(v1)\n",
    "\n",
    "    #vector_x = np.cross(vec1, vec_x)/np.linalg.norm(np.cross(vec1, vec_x))\n",
    "    angle_x = (math.acos(np.dot(vec1, vec_x)))\n",
    "\n",
    "    #vector_y = np.cross(vec1, vec_y)/np.linalg.norm(np.cross(vec1, vec_y))\n",
    "    angle_y = (math.acos(np.dot(vec1, vec_y)))\n",
    "\n",
    "    vector_z = np.cross(vec1, vec_z)/np.linalg.norm(np.cross(vec1, vec_z))\n",
    "    angle_z = -(math.acos(np.dot(vec1, vec_z)))  #<<<Changed to adapt\n",
    "\n",
    "    #Rotation = filtered_Pt_Cloud.get_rotation_matrix_from_axis_angle(angle*vector) #alternative Open3D lib.\n",
    "    Rotation = R.from_rotvec(angle_z*vector_z)\n",
    "    \n",
    "    return Rotation, angle_x, angle_y, angle_z\n",
    "\n",
    "\n",
    "def Bounds_gen(minB, maxB, spacing):\n",
    "    ctr = 0\n",
    "    bounds = np.array([[0.,0.]])\n",
    "    CurrB = minB\n",
    "\n",
    "    while CurrB < maxB-0.5*spacing: #from left most to right most\n",
    "\n",
    "        LowerB = minB + ctr * spacing #if we shift the X or Y coordinates in multiples of spacing, we should get different lines.\n",
    "        if ctr == 0:\n",
    "            CurrB = LowerB + spacing/2 #lower + spacing/2 only for first condition\n",
    "        else:\n",
    "            CurrB = LowerB + spacing #lower + spacing\n",
    "        bounds = np.append(bounds,[[LowerB,CurrB]], axis=0)\n",
    "        #print(\"CurrB:\",CurrB,\"MaxB:\",maxB,\"diff:\",maxB-CurrB)\n",
    "        ctr+= 1\n",
    "    bounds = np.delete(bounds, 0, axis=0)\n",
    "    return bounds\n",
    "\n",
    "def cropped_PC(original_PC, spacing, X=0, idx = 0, centered = True):\n",
    "    PC_BBOX = original_PC.get_axis_aligned_bounding_box()\n",
    "    minB_X = PC_BBOX.min_bound[0]\n",
    "    maxB_X = PC_BBOX.max_bound[0]\n",
    "    minB_Y = PC_BBOX.min_bound[1]\n",
    "    maxB_Y = PC_BBOX.max_bound[1]\n",
    "    minB_Z = PC_BBOX.min_bound[2]\n",
    "    maxB_Z = PC_BBOX.max_bound[2]\n",
    "    nor = np.array(original_PC.normals)\n",
    "    pts = np.array(original_PC.points)\n",
    "\n",
    "    Xs,Ys = get_XY_angles_from_PC(nor)\n",
    "    X_dev = np.std(Xs)\n",
    "    Y_dev = np.std(Ys)\n",
    "    print(\"X Standard Dev:\", X_dev, \"    Y Standard Dev:\", Y_dev)\n",
    "    \n",
    "    if(X > 1):\n",
    "        if(X==2): #Manual override at 2\n",
    "            X = 0  \n",
    "            print(\"Manual Override! taking curve around World Y-axis\",\"X=\", X)\n",
    "        elif(X==3):  #Manual override at 3\n",
    "            X = 1\n",
    "            print(\"Manual Override! taking curve around World X-axis\",\"X=\", X)\n",
    "        else: X = 1\n",
    "    else:\n",
    "        \n",
    "        if (X_dev <= Y_dev): \n",
    "            X = 1\n",
    "            print(\"Object's curve around World X-axis\",\"X=\", X)\n",
    "        elif (X_dev > Y_dev): \n",
    "            X = 0\n",
    "            print(\"Object's curve around World Y-axis\",\"X=\", X)\n",
    "\n",
    "    if X == 1: #X sided sweep crop\n",
    "        bounds = Bounds_gen(minB_X, maxB_X, spacing)\n",
    "        print(\"Total bounds:\",len(bounds))\n",
    "        if centered == True:\n",
    "            idx = int(np.floor(len(bounds)/2))\n",
    "        else:\n",
    "            if idx > len(bounds)-1:\n",
    "                print(\"Warning! Index value out of bounds, setting to max value\")\n",
    "                idx = -1\n",
    "        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(bounds[idx][0], minB_Y, minB_Z), max_bound=(bounds[idx][1], maxB_Y, maxB_Z))\n",
    "\n",
    "        result = original_PC.crop(bbox)\n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "\n",
    "        ind = np.argsort(res_pts[:, 1]) #Sort Y coordinates from lowest to highest, X values are almost constant\n",
    "        res_pts = res_pts[ind]  #no longer required to change values to negative \n",
    "        res_nor = res_nor[ind]\n",
    "\n",
    "    elif X == 0: #Y sided sweep crop\n",
    "        bounds = Bounds_gen(minB_Y, maxB_Y, spacing)\n",
    "        print(\"Total bounds:\",len(bounds))\n",
    "        if centered == True:\n",
    "            idx = int(np.floor(len(bounds)/2))\n",
    "        else:\n",
    "            if idx > len(bounds)-1:\n",
    "                print(\"Warning! Index value out of bounds, setting to max value\")\n",
    "                idx = -1\n",
    "        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(minB_X, bounds[idx][0], minB_Z), max_bound=(maxB_X, bounds[idx][1], maxB_Z)) \n",
    "        print(bbox)\n",
    "\n",
    "        result = original_PC.crop(bbox)\n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "\n",
    "        ind = np.argsort(res_pts[:, 0]) #Sort X coordinates from lowest to highest, Y values are almost constant\n",
    "        res_pts = res_pts[ind]\n",
    "        #res_pts[:,2] *=-1  #Change z values to negative\n",
    "        res_nor = res_nor[ind]\n",
    "\n",
    "    sorted_pointcloud = o3d.geometry.PointCloud()\n",
    "    sorted_pointcloud.points = o3d.utility.Vector3dVector(res_pts)\n",
    "    sorted_pointcloud.normals = o3d.utility.Vector3dVector(res_nor)\n",
    "    return sorted_pointcloud\n",
    "\n",
    "\n",
    "def generate_coordinates(point_cloud):\n",
    "    Coordinates = np.array([[0.,0.,0.,0.,0.,0.,0.]])\n",
    "    world_Coordinates = np.array([[0.,0.,0.,0.,0.,0.,0.]])\n",
    "    point_cloud_pts = np.array(point_cloud.points)\n",
    "    point_cloud_nor = np.array(point_cloud.normals)\n",
    "    \n",
    "    for index in range (len(point_cloud_pts)):\n",
    "\n",
    "        rotat, angle_x, angle_y, angle_z = getRotation(-point_cloud_nor[index])  #<<<changed here pts -ve\n",
    "\n",
    "        a1 = rotat.as_matrix()[0][0]\n",
    "        a2 = rotat.as_matrix()[0][1]\n",
    "        a3 = rotat.as_matrix()[0][2]\n",
    "        b1 = rotat.as_matrix()[1][0]\n",
    "        b2 = rotat.as_matrix()[1][1]\n",
    "        b3 = rotat.as_matrix()[1][2]\n",
    "        c1 = rotat.as_matrix()[2][0]\n",
    "        c2 = rotat.as_matrix()[2][1]\n",
    "        c3 = rotat.as_matrix()[2][2]\n",
    "\n",
    "        KDL_original_plane_frame = PyKDL.Frame(PyKDL.Rotation(a1,a2,a3, b1,b2,b3, c1,c2,c3),\n",
    "                                               PyKDL.Vector(point_cloud_pts[index][0],\n",
    "                                                            point_cloud_pts[index][1],\n",
    "                                                             point_cloud_pts[index][2]))\n",
    "\n",
    "        KDL_flip_frame = PyKDL.Frame(PyKDL.Rotation.RPY(0, np.pi, 0), PyKDL.Vector(0,0,0)) #mirror plane frame to match camera frame\n",
    "        KDL_final_frame = KDL_original_plane_frame * KDL_flip_frame\n",
    "        KDL_trans = KDL_final_frame.p\n",
    "        KDL_ROT_quat = KDL_final_frame.M.GetQuaternion() \n",
    "\n",
    "        Coordinates = np.append(Coordinates,[[KDL_trans[0] ,KDL_trans[1],KDL_trans[2] ,KDL_ROT_quat[0] \n",
    "                                     ,KDL_ROT_quat[1] ,KDL_ROT_quat[2],KDL_ROT_quat[3]]],axis=0)\n",
    "\n",
    "        Publish_coordinates([[KDL_trans[0] ,KDL_trans[1],KDL_trans[2] ,KDL_ROT_quat[0] \n",
    "                            ,KDL_ROT_quat[1] ,KDL_ROT_quat[2],KDL_ROT_quat[3]]], \n",
    "                            'camera_depth_optical_frame', 'plane', static = True)\n",
    "\n",
    "        transform_plane = fetch_transform(tfbuffer,'world', 'plane_static_0',quat=1)\n",
    "\n",
    "        world_Coordinates = np.append(world_Coordinates,[[transform_plane[0] ,transform_plane[1]\n",
    "                                            ,transform_plane[2] ,transform_plane[3] ,transform_plane[4] \n",
    "                                             ,transform_plane[5] ,transform_plane[6]]],axis=0)\n",
    "\n",
    "    Coordinates = np.delete(Coordinates, 0, axis=0)\n",
    "    world_Coordinates2 = np.delete(world_Coordinates, 0, axis=0)\n",
    "    return Coordinates, world_Coordinates2\n",
    "\n",
    "def Publish_coordinates(Coordinates, parent_name, child_name, static = False): #Coordinates expects [[0,0,0,0,0,0,0],[1,1,1,1,1,1,1]] format\n",
    "\n",
    "    for index in range (len(Coordinates)):\n",
    "        \n",
    "        if static==True:\n",
    "            static_t.header.stamp = rospy.Time.now()\n",
    "            static_t.header.frame_id = parent_name #\"camera_depth_optical_frame\"\n",
    "            static_t.child_frame_id = child_name+\"_static_\"+str(index)\n",
    "            static_t.transform.translation.x = Coordinates[index][0]\n",
    "            static_t.transform.translation.y = Coordinates[index][1]\n",
    "            static_t.transform.translation.z = Coordinates[index][2]\n",
    "\n",
    "            #r_quat = tf.transformations.quaternion_from_euler(Roll,Pitch,Yaw)\n",
    "            static_t.transform.rotation.x = Coordinates[index][3]\n",
    "            static_t.transform.rotation.y = Coordinates[index][4]\n",
    "            static_t.transform.rotation.z = Coordinates[index][5]\n",
    "            static_t.transform.rotation.w = Coordinates[index][6]\n",
    "            static_br.sendTransform(static_t)\n",
    "        else:\n",
    "            t.header.frame_id = parent_name #\"camera_depth_optical_frame\"\n",
    "            t.child_frame_id = child_name+\"_\"+str(index)\n",
    "\n",
    "            t.header.stamp = rospy.Time.now()\n",
    "            t.transform.translation.x = Coordinates[index][0]\n",
    "            t.transform.translation.y = Coordinates[index][1]\n",
    "            t.transform.translation.z = Coordinates[index][2] \n",
    "\n",
    "            #r_quat = tf.transformations.quaternion_from_euler(Roll,Pitch,Yaw)\n",
    "            t.transform.rotation.x = Coordinates[index][3]\n",
    "            t.transform.rotation.y = Coordinates[index][4]\n",
    "            t.transform.rotation.z = Coordinates[index][5]\n",
    "            t.transform.rotation.w = Coordinates[index][6]\n",
    "            br.sendTransform(t)\n",
    "\n",
    "\n",
    "def Cluster_Point_Cloud(original_PC, eps=0.02, min_points=10):\n",
    "\n",
    "    labels = np.array(original_PC.cluster_dbscan(eps=eps, min_points=min_points))\n",
    "    uniques = np.unique(labels)\n",
    "    clouds = np.array(original_PC)\n",
    "    \n",
    "    if ((uniques[0] == -1) and (len(uniques) == 1)):\n",
    "        clouds = np.append(clouds,[original_PC])\n",
    "\n",
    "    else:\n",
    "        for i in range (len(uniques)):\n",
    "\n",
    "            if uniques[i] > -1:\n",
    "                idx = np.where(labels==uniques[i])[0]\n",
    "                cluster_pcd = original_PC.select_by_index(idx, invert=False)\n",
    "                clouds = np.append(clouds,[cluster_pcd])\n",
    "\n",
    "    clouds = np.delete(clouds, 0)\n",
    "    print(\"Number of clusters:\", len(clouds))\n",
    "    return clouds\n",
    "\n",
    "\n",
    "def go_to_coord_goal(move_group,coord):\n",
    "    pose_goal = geometry_msgs.msg.Pose()\n",
    "    pose_goal.position.x = coord[0]\n",
    "    pose_goal.position.y = coord[1]\n",
    "    pose_goal.position.z = coord[2]\n",
    "    pose_goal.orientation.x = coord[3]\n",
    "    pose_goal.orientation.y = coord[4]\n",
    "    pose_goal.orientation.z = coord[5]\n",
    "    pose_goal.orientation.w = coord[6]\n",
    "\n",
    "    move_group.set_pose_target(pose_goal)\n",
    "    \n",
    "    success = move_group.go(wait=True)\n",
    "    \n",
    "    move_group.stop()\n",
    "    move_group.clear_pose_targets()\n",
    "    current_pose = move_group.get_current_pose().pose\n",
    "\n",
    "def array_to_data(array):\n",
    "    im = PIL_img.fromarray(array)\n",
    "    output_buffer = BytesIO()\n",
    "    im.save(output_buffer, format=\"PNG\")\n",
    "    data = output_buffer.getvalue()\n",
    "    return data\n",
    "\n",
    "def fetch_cloud_image(pointCloud, RX=0, RY=0, RZ=0):\n",
    "    mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1,origin=[0, 0, 0])\n",
    "    mesh_pc= mesh.sample_points_uniformly(number_of_points=1000000, use_triangle_normal=False)\n",
    "    tmp_cloud = copy.deepcopy(pointCloud)  #To avoid overwriting the original point cloud\n",
    "    \n",
    "    tmp_cloud+=mesh_pc\n",
    "    tmp_Rot = pointCloud.get_rotation_matrix_from_xyz((np.radians(RX), np.radians(RY), np.radians((RZ))))\n",
    "    \n",
    "    tmp_cloud.rotate(tmp_Rot, center=(0, 0, 0))\n",
    "    \n",
    "    vis = o3d.visualization.Visualizer() \n",
    "    vis.create_window(visible=False, width=640, height=480) \n",
    "    vis.add_geometry(tmp_cloud) \n",
    "    vis.poll_events() \n",
    "    vis.update_renderer() \n",
    "    color = vis.capture_screen_float_buffer(True) \n",
    "    #time.sleep(5)\n",
    "    vis.destroy_window() \n",
    "    #color = np.asarray(color)\n",
    "    color = (255.0 * np.asarray(color)).astype(np.uint8)\n",
    "    color = array_to_data(color) #Format according to the GUI requirements\n",
    "    return color\n",
    "\n",
    "def Cluster_selection_gui(clouds):\n",
    "    dat=[]  #to hold cluster's images\n",
    "    pcd_combined = o3d.geometry.PointCloud()\n",
    "    \n",
    "    for current_cloud in clouds:\n",
    "            color = fetch_cloud_image(current_cloud, RX=120,RZ=180)  #different point clouds to be viewed, set view by RX,RY,RZ\n",
    "            dat.append(color)\n",
    "    result = subprocess.run([sys.executable,  \"Cluster_selection_gui.py\"],capture_output=True,text=True,check=True,shell=False, input=repr(dat))   \n",
    "    OP = literal_eval(result.stdout)\n",
    "    selected_PC = OP[0]\n",
    "    selected_mode = OP[1]\n",
    "    \n",
    "    for k in selected_PC:\n",
    "        pcd_combined += clouds[k]  #combine selected pointclouds \n",
    "    return pcd_combined, selected_mode\n",
    "\n",
    "#def Convert_message(string):\n",
    "#    return [int(s) for s in re.findall('[0-9]', string)]\n",
    "\n",
    "def get_XY_angles_from_PC(tst_downpcd_nor):\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    vec_x = [1.,0.,0.] \n",
    "    vec_y = [0.,1.,0.]\n",
    "    \n",
    "    for normal_vec in tst_downpcd_nor:\n",
    "        if np.all(normal_vec==[0., 0., 1.]): normal_vec = [0, 0.000001, 0.999999]\n",
    "        vec1 = normal_vec / np.linalg.norm(normal_vec)\n",
    "        angle_x = np.round(np.degrees((math.acos(np.dot(vec1, vec_x)))))\n",
    "        angle_y = np.round(np.degrees((math.acos(np.dot(vec1, vec_y)))))\n",
    "        Xs.append(angle_x)\n",
    "        Ys.append(angle_y)\n",
    "    Xs = np.array(Xs)\n",
    "    Ys = np.array(Ys)\n",
    "    return Xs, Ys\n",
    "\n",
    "\n",
    "print(\"Loaded!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d6be761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to initial position\n"
     ]
    }
   ],
   "source": [
    "#move_group.get_current_pose()\n",
    "initial_coordinates = [0.408,0.,0.834,0.9238709648606045,-0.3826807475252162,-0.003837162229149499,0.001778186465088758]\n",
    "\n",
    "initial_coordinates_vertical = [0.28701854121331966,-0.03260223409115588,0.44804421741139216,\n",
    "                       -0.6945874945768339,0.29634459105695476,-0.6174160637305418,0.2202850425613284]\n",
    "\n",
    "move_group.get_current_pose()\n",
    "\n",
    "go_to_coord_goal(move_group, initial_coordinates) #pass values to function to make robot move in cartesian space. \n",
    "print(\"Moving to initial position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6d302cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color frame: (480, 640, 3)  Depth frame: (480, 640)\n",
      "556\n",
      "Center of image: 556 mm\n",
      "Recompute the normal of the downsampled point cloud\n",
      "Align normals towards camera\n",
      "AxisAlignedBoundingBox: min: (-0.755789, -0.609844, -1.58657), max: (1.34372, 0.976633, -0.159617)\n",
      "PointCloud with 546 points.\n",
      "\n",
      "0.011475102871330555\n",
      "471\n",
      "\n",
      "Center Coordinates(ground truth): [-0.02632869 -0.04871436  0.54905188]\n",
      "Points Coordinates(estimated center point): [-0.02742619 -0.04715836  0.53773586]\n",
      "Normal Coordinates(Normal of estimated center point): [ 0.12867676  0.10779038 -0.9858111 ]\n"
     ]
    }
   ],
   "source": [
    "spacing = 0.01 #spacing between each point in point cloud in meters (use this to approximate the shape of surface)\n",
    "offset_y = 0.13 #distance to crop in world's X axis away from robot, to avoid robot's shadow appearing in PC.\n",
    "offset_z = 0.9\n",
    "mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1,origin=[0, 0, 0])\n",
    "\n",
    "point = (320, 240)\n",
    "\n",
    "\n",
    "#_, depth_frame, color_frame = dc.get_frame()\n",
    "color_frame, depth_frame = grab_frame()\n",
    "print(\"color frame:\",color_frame.shape, \" Depth frame:\",depth_frame.shape)\n",
    "distance = depth_frame[point[1], point[0]]\n",
    "print(distance)\n",
    "print(\"Center of image:\", distance, \"mm\")\n",
    "\n",
    "pt_cloud = Generate_PointCloud(color_frame,depth_frame,from_depth=False)\n",
    "\n",
    "\n",
    "downpcd = pt_cloud.voxel_down_sample(voxel_size=spacing)\n",
    "\n",
    "print(\"Recompute the normal of the downsampled point cloud\")\n",
    "downpcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.06, max_nn=30)) #radius in meters\n",
    "print(\"Align normals towards camera\")\n",
    "downpcd.orient_normals_towards_camera_location(camera_location=[0, 0, 0])\n",
    "\n",
    "#o3d.visualization.draw_geometries([downpcd,mesh], point_show_normal=True)\n",
    "\n",
    "## Filter out hidden points\n",
    "diameter = np.linalg.norm(np.asarray(downpcd.get_max_bound()) - np.asarray(downpcd.get_min_bound()))\n",
    "cam = [0, 0, diameter]\n",
    "radius = diameter * 1000\n",
    "_, pt_map = downpcd.hidden_point_removal(cam, radius) #Get all points that are visible from given view point\n",
    "downpcd = downpcd.select_by_index(pt_map)\n",
    "\n",
    "o3d.visualization.draw_geometries([downpcd,mesh], point_show_normal=True)\n",
    "\n",
    "#v2_camera = np.array(downpcd.normals)\n",
    "v2_camera_pts = np.array(downpcd.points)\n",
    "\n",
    "\n",
    "#idx = np.where(abs(v2_camera_pts[:,2]) < (distance/1000)+0.1)[0] #fetch all indexes of values less than distance of center of img 0.3 in column 3 of rec\n",
    "idx = np.where(abs(v2_camera_pts[:,2]) < abs(np.min(v2_camera_pts[:,2]))-0.06)[0] #fetch all indexes of values less than distance of center of img 0.3 in column 3 of rec\n",
    "\n",
    "#print(idx)\n",
    "filtered_Pt_Cloud = downpcd.select_by_index(idx, invert=False)\n",
    "\n",
    "\n",
    "#CROP TO FILTER OUT ROBOT'S SHADOW ADJUST OFFSET ACCORDINGLY\n",
    "PC_BBOX = filtered_Pt_Cloud.get_axis_aligned_bounding_box()\n",
    "print(PC_BBOX)\n",
    "minB_X = PC_BBOX.min_bound[0]\n",
    "maxB_X = PC_BBOX.max_bound[0]\n",
    "minB_Y = PC_BBOX.min_bound[1]\n",
    "maxB_Y = PC_BBOX.max_bound[1]\n",
    "minB_Z = PC_BBOX.min_bound[2]\n",
    "maxB_Z = PC_BBOX.max_bound[2]\n",
    "bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(minB_X, minB_Y, minB_Z+offset_z), max_bound=(maxB_X, maxB_Y-offset_y, maxB_Z)) \n",
    "filtered_Pt_Cloud = filtered_Pt_Cloud.crop(bbox)\n",
    "\n",
    "Re = filtered_Pt_Cloud.get_rotation_matrix_from_xyz((np.pi, 0 , 0))\n",
    "filtered_Pt_Cloud = filtered_Pt_Cloud.rotate(Re, center=(0,0,0))\n",
    "\n",
    "print(filtered_Pt_Cloud)\n",
    "o3d.visualization.draw_geometries([filtered_Pt_Cloud,mesh], point_show_normal=True)\n",
    "\n",
    "\n",
    "\n",
    "nor = np.array(filtered_Pt_Cloud.normals)\n",
    "pts = np.array(filtered_Pt_Cloud.points)\n",
    "\n",
    "print()\n",
    "distance,index = spatial.KDTree(pts).query( filtered_Pt_Cloud.get_center() ) #find coordinates \n",
    "                                                                             #that are closest to the center \n",
    "\n",
    "print(distance)\n",
    "print(index)\n",
    "print()\n",
    "print(\"Center Coordinates(ground truth):\",filtered_Pt_Cloud.get_center())\n",
    "print(\"Points Coordinates(estimated center point):\",pts[index])\n",
    "print(\"Normal Coordinates(Normal of estimated center point):\",nor[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b51d7",
   "metadata": {},
   "source": [
    "### Clustering of points in Point cloud\n",
    "ref: http://www.open3d.org/docs/release/tutorial/geometry/pointcloud.html#DBSCAN-clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e32e8",
   "metadata": {},
   "source": [
    "### Crop point cloud to get a points in a single line in X or Y axis, \n",
    "*Use idx to give offset else use centered to get the points from center of the object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d343540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 1\n",
      "X Standard Dev: 32.23175566252703     Y Standard Dev: 3.319452849135058\n",
      "Object's curve around World Y-axis X= 0\n",
      "Total bounds: 16\n",
      "AxisAlignedBoundingBox: min: (-0.129168, -0.0541229, 0.532244), max: (0.0602247, -0.0441229, 0.604833)\n"
     ]
    }
   ],
   "source": [
    "clouds = Cluster_Point_Cloud(filtered_Pt_Cloud, eps=0.05, min_points=10)\n",
    "\n",
    "result,selected_mode = Cluster_selection_gui(clouds) #Will present a GUI to select all relevant pointclouds\n",
    "#o3d.visualization.draw_geometries([result, mesh], point_show_normal=True)\n",
    "\n",
    "result = cropped_PC(result, spacing, X=selected_mode, idx = 0, centered = True)\n",
    "o3d.visualization.draw_geometries([result, mesh], point_show_normal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf797ad",
   "metadata": {},
   "source": [
    "### This will generate and boradcast all points in pointcloud to tf2 or tf2_static topic, visualize using Rviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c84aed30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total coordinates: 30\n"
     ]
    }
   ],
   "source": [
    "cam_coords, world_coords = generate_coordinates(result)\n",
    "#cam_coords, world_coords = generate_coordinates(clouds[0])\n",
    "print(\"Total coordinates:\",len(world_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f757d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Publish_coordinates(coords,'camera_depth_optical_frame','plane' static = False)\n",
    "for i in range (len(world_coords)):\n",
    "    Publish_coordinates([world_coords[i]],'world','plane', static = True)\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f63a8",
   "metadata": {},
   "source": [
    "## Robot Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edb115e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5169460752600362, -0.2931378109959326, 0.513008685939273, -0.6681884199802429, 0.6700613747685901, 0.21741105176519843, 0.23932075551001514]\n",
      "Moving to Target\n",
      "[0.48275791842468135, 0.3029990461584236, 0.5424310002008139, 0.6723376049115928, -0.6706358336686491, 0.2510431018299105, 0.18758220769816047]\n",
      "Moving to Target\n",
      "[0.47886321568535833, 0.28336987425503724, 0.556188698573584, 0.6753621328780705, -0.6737497771799281, 0.2450104375878842, 0.17296564022190705]\n",
      "Moving to Target\n",
      "[0.4740081452637322, 0.2124820160832368, 0.597251793515541, 0.6911346647084695, -0.6900721842291394, 0.18922248798458868, 0.1016272888178102]\n",
      "Moving to Target\n",
      "[0.4770829722063061, 0.12467244219755635, 0.622697406949539, 0.7037995883473983, -0.7034794880516034, 0.09634621581564563, 0.02236416845516313]\n",
      "Moving to Target\n",
      "[0.48181057100605745, -0.03133488004538708, 0.6199474026018607, -0.7015602473449096, 0.7024256668774993, 0.04724339852003739, 0.11036060507865131]\n",
      "Moving to Target\n",
      "[0.48332572734072266, -0.07713577031598905, 0.6054877095729243, -0.6958199971133036, 0.6970266920142646, 0.08879053955740218, 0.14867603143075678]\n",
      "Moving to Target\n",
      "[0.48768885020750663, -0.12738227876263175, 0.5852721024223471, -0.6872786702329811, 0.6888462239288992, 0.13576029565719192, 0.18630096979015928]\n",
      "Moving to Target\n",
      "[0.4923171104344839, -0.21346419902926872, 0.5263767340613448, -0.6576041425243501, 0.6599788195622639, 0.23431217879459432, 0.27763744764088366]\n",
      "Moving to Target\n",
      "[0.4914574363555423, -0.25771258330100727, 0.4798060418321829, -0.6376660550523248, 0.640434791369043, 0.28048608524463997, 0.32334631004888953]\n",
      "Moving to Target\n"
     ]
    }
   ],
   "source": [
    "z_offset = 0.3\n",
    "\n",
    "Publish_coordinates([world_coords[0]],'world','plane', static = True)\n",
    "time.sleep(5)\n",
    "\n",
    "for id_x in range(0,len(world_coords),3):  #Take every 4th point in coordinates\n",
    "    transform_world_plane = world_coords[id_x]\n",
    "    KDL_original_plane_frame = PyKDL.Frame(PyKDL.Rotation.Quaternion(transform_world_plane[3],\n",
    "                                                                     transform_world_plane[4], \n",
    "                                                                     transform_world_plane[5], \n",
    "                                                                     transform_world_plane[6]),\n",
    "                                           PyKDL.Vector(transform_world_plane[0], \n",
    "                                                        transform_world_plane[1], \n",
    "                                                        transform_world_plane[2]))\n",
    "\n",
    "    trans_x,trans_y,trans_z = KDL_original_plane_frame * PyKDL.Vector(0, 0, z_offset) #Add offset\n",
    "    KDL_original_plane_frame.p = PyKDL.Vector(trans_x,trans_y,trans_z) #update original plane frame to new location\n",
    "\n",
    "    KDL_flip_frame = PyKDL.Frame(PyKDL.Rotation.RPY(np.pi, 0.,np.pi), PyKDL.Vector(0, 0, 0)) #mirror plane frame to match camera frame\n",
    "\n",
    "    KDL_final_frame = KDL_original_plane_frame * KDL_flip_frame\n",
    "\n",
    "    KDL_trans = KDL_final_frame.p\n",
    "    KDL_ROT_quat = KDL_final_frame.M.GetQuaternion() \n",
    "\n",
    "    final_coordinates = [KDL_trans[0], KDL_trans[1], KDL_trans[2], KDL_ROT_quat[0], KDL_ROT_quat[1], \n",
    "                                                                   KDL_ROT_quat[2], KDL_ROT_quat[3]]\n",
    "\n",
    "    print(final_coordinates)\n",
    "\n",
    "    Publish_coordinates([final_coordinates], \"world\", 'Camera_Target', static = True)\n",
    "    \n",
    "    #Fetch transform between link8 and optical depth cam\n",
    "    transform_link8_camera_depth = fetch_transform(tfbuffer,'camera_depth_optical_frame', 'panda_link8',quat=1)\n",
    "\n",
    "    KDL_link8_cam_frame = PyKDL.Frame(PyKDL.Rotation.Quaternion(transform_link8_camera_depth[3],\n",
    "                                                                transform_link8_camera_depth[4], \n",
    "                                                                transform_link8_camera_depth[5],\n",
    "                                                                transform_link8_camera_depth[6]),\n",
    "                                      PyKDL.Vector(transform_link8_camera_depth[0], \n",
    "                                                   transform_link8_camera_depth[1], \n",
    "                                                   transform_link8_camera_depth[2])) #link8-camera frame\n",
    "\n",
    "    #Offset original frame by z offset, flip the frame to match camera, multiply with link8-camera frame to replicate\n",
    "    #pose between camera and link8\n",
    "    KDL_final_frame = KDL_original_plane_frame * KDL_flip_frame * KDL_link8_cam_frame \n",
    "\n",
    "    KDL_trans = KDL_final_frame.p\n",
    "    KDL_ROT_quat = KDL_final_frame.M.GetQuaternion() \n",
    "\n",
    "    final_coordinates = [KDL_trans[0], KDL_trans[1], KDL_trans[2], KDL_ROT_quat[0], KDL_ROT_quat[1], \n",
    "                                                                   KDL_ROT_quat[2], KDL_ROT_quat[3]]\n",
    "    Publish_coordinates([final_coordinates], \"world\", 'EEF_Target', static = True)\n",
    "\n",
    "    ## Move Robot to TARGET\n",
    "    go_to_coord_goal(move_group, final_coordinates) #pass values to function to make robot move in cartesian space. \n",
    "    print(\"Moving to Target\")\n",
    "    #time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea0e864f",
   "metadata": {},
   "source": [
    "print(\"============ Printing robot current Joint values\") #O/P: current joint values of the end-effector\n",
    "#print(move_group.get_current_joint_values())\n",
    "print(\"In Degrees: \", np.degrees( move_group.get_current_joint_values() ) ) #O/P joint angles in degrees\n",
    "print(\"\")\n",
    "\n",
    "print(\"============ Printing robot current pose\") #O/P will be XYZ and xyz w  positions and orientations of the robot.\n",
    "print(move_group.get_current_pose())\n",
    "print(\"\")\n",
    "\n",
    "print(\"============ Printing robot current RPY\") #O/P wil be orientation of end-effector in RPY (Radians)\n",
    "#print(move_group.get_current_rpy())\n",
    "print(\"RPY In Degrees: \", np.degrees( move_group.get_current_rpy() ) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
