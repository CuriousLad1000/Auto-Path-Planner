{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sensor_msgs.msg import Image, CameraInfo\n",
    "from realsense2_camera.msg import Extrinsics\n",
    "from cv_bridge import CvBridge\n",
    "import rospy\n",
    "\n",
    "import pyrealsense2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import open3d as o3d\n",
    "from scipy import spatial\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import math\n",
    "import tf2_ros, tf\n",
    "import geometry_msgs.msg\n",
    "import PyKDL\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import moveit_commander\n",
    "import moveit_msgs.msg\n",
    "from moveit_commander.conversions import pose_to_list\n",
    "\n",
    "import subprocess\n",
    "from ast import literal_eval\n",
    "import copy\n",
    "from PIL import Image as PIL_img\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "image = None\n",
    "\n",
    "\n",
    "rospy.init_node('my_pic', anonymous=True)\n",
    "\n",
    "moveit_commander.roscpp_initialize(sys.argv)\n",
    "robot = moveit_commander.RobotCommander()\n",
    "scene = moveit_commander.PlanningSceneInterface()\n",
    "group_name = \"panda_manipulator\"  #\"panda_arm\"\n",
    "move_group = moveit_commander.MoveGroupCommander(group_name) #we'll pass it on while calling functions\n",
    "\n",
    "\n",
    "tfbuffer = tf2_ros.Buffer()\n",
    "listener = tf2_ros.TransformListener(tfbuffer)\n",
    "br = tf2_ros.TransformBroadcaster()\n",
    "t = geometry_msgs.msg.TransformStamped()\n",
    "\n",
    "static_br = tf2_ros.StaticTransformBroadcaster()\n",
    "static_t = geometry_msgs.msg.TransformStamped()\n",
    "\n",
    "bridge = CvBridge()\n",
    "loop_rate = rospy.Rate(0.5) # Node cycle rate (in Hz).\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Initialize Camera Intel Realsense\n",
    "#dc = DepthCamera()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_frame():\n",
    "   \n",
    "    frame_color=rospy.wait_for_message('/camera/color/image_raw', Image, timeout=None) #wait_for_message(topic, topic_type, timeout=None): \n",
    "    cv_image_color = bridge.imgmsg_to_cv2(frame_color, desired_encoding='rgb8')\n",
    "    frame_depth = rospy.wait_for_message('/camera/depth/image_rect_raw', Image, timeout=None) #wait_for_message(topic, topic_type, timeout=None): \n",
    "    cv_image_depth = bridge.imgmsg_to_cv2(frame_depth)\n",
    "    \n",
    "    return cv_image_color, cv_image_depth\n",
    "\n",
    "def get_cam_param(depth_to_color=False):\n",
    "    frame_depth_info=rospy.wait_for_message('/camera/depth/camera_info', CameraInfo, timeout=None)\n",
    "    \n",
    "    w = frame_depth_info.width\n",
    "    h = frame_depth_info.height\n",
    "    fx = frame_depth_info.K[0]\n",
    "    fy = frame_depth_info.K[4]\n",
    "    cx = frame_depth_info.K[2]\n",
    "    cy = frame_depth_info.K[5]\n",
    "    \n",
    "    if depth_to_color:\n",
    "        frame_depth_extrinsic=rospy.wait_for_message('/camera/extrinsics/depth_to_color', Extrinsics, timeout=None)\n",
    "        return w,h,fx,fy,cx,cy,frame_depth_extrinsic.translation,frame_depth_extrinsic.rotation\n",
    "    else:\n",
    "        rot = [1,0,0, 0,1,0, 0,0,1]\n",
    "        trans = [0, 0, 0]\n",
    "        return w,h,fx,fy,cx,cy, trans, rot\n",
    "\n",
    "def Generate_PointCloud(color_frame,depth_frame,from_depth=False,depth_scale = 1000.0, depth_trunc=1.0,align=False): #Truncate distances at 1Meter default\n",
    "\n",
    "    w,h,fx,fy,cx,cy, trans, rot = get_cam_param(depth_to_color=align)   \n",
    "\n",
    "    cam = o3d.camera.PinholeCameraParameters()\n",
    "    cam.intrinsic = o3d.camera.PinholeCameraIntrinsic(w, h, fx,fy, cx, cy)\n",
    "\n",
    "    cam.extrinsic = np.array([[rot[0], rot[1], rot[2], trans[0]], \n",
    "                              [rot[3], rot[4], rot[5], trans[1]], \n",
    "                              [rot[6], rot[7], rot[8], trans[2]], \n",
    "                              [0.    , 0.    , 0.    , 1.      ]])\n",
    "    \n",
    "\n",
    "    color_raw = o3d.geometry.Image(np.asarray(color_frame))\n",
    "    depth_raw = o3d.geometry.Image(np.asarray(depth_frame.astype(np.uint16)))\n",
    "    \n",
    "    if from_depth==False:\n",
    "        rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(color_raw, depth_raw, convert_rgb_to_intensity=False)\n",
    "        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, cam.intrinsic, cam.extrinsic)\n",
    "        #CROP TO FILTER OUT ROBOT'S SHADOW ADJUST OFFSET ACCORDINGLY\n",
    "        PC_BBOX = pcd.get_axis_aligned_bounding_box()\n",
    "        minB_X = PC_BBOX.min_bound[0]\n",
    "        maxB_X = PC_BBOX.max_bound[0]\n",
    "        minB_Y = PC_BBOX.min_bound[1]\n",
    "        maxB_Y = PC_BBOX.max_bound[1]\n",
    "        minB_Z = PC_BBOX.min_bound[2]\n",
    "        maxB_Z = PC_BBOX.max_bound[2]\n",
    "\n",
    "        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(minB_X, minB_Y, minB_Z), max_bound=(maxB_X, maxB_Y, maxB_Z-depth_trunc)) \n",
    "        pcd = pcd.crop(bbox)\n",
    "        #pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, cam.intrinsic)\n",
    "    else:\n",
    "        pcd =  o3d.geometry.PointCloud.create_from_depth_image(depth_raw, cam.intrinsic, cam.extrinsic, \n",
    "                                                               depth_scale, depth_trunc,\n",
    "                                                               stride=1, project_valid_depth_only=True)\n",
    "    \n",
    "    pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]) # Flip it, otherwise the pointcloud will be upside down.\n",
    "    return pcd\n",
    "\n",
    "\n",
    "def fetch_transform(tfbuffer,frame1,frame2,quat=0):\n",
    "    flag = 0\n",
    "    while flag==0:\n",
    "        try:\n",
    "            trans = tfbuffer.lookup_transform(frame1, frame2, rospy.Time(),rospy.Duration(8.0))\n",
    "            #print (trans)\n",
    "            trans = trans.transform  #save translation and rotation\n",
    "            #rot = PyKDL.Rotation.Quaternion(* [ trans.rotation.x,trans.rotation.y,trans.rotation.z,trans.rotation.w] )\n",
    "            #ypr = [ i  / np.pi * 180 for i in rot.GetEulerZYX() ]\n",
    "            #print(ypr[2],ypr[1],ypr[0])\n",
    "            \n",
    "            rot = R.from_quat([ trans.rotation.x,trans.rotation.y,trans.rotation.z,trans.rotation.w]) #creates rotation matrix\n",
    "            rpy = rot.as_euler('XYZ',degrees=True) #extrinsic\n",
    "            break\n",
    "        except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException) as e:\n",
    "            #print (\"Fail\", e)\n",
    "            continue\n",
    "    if quat==0:\n",
    "        return trans.translation.x, trans.translation.y, trans.translation.z, rpy[0],rpy[1],rpy[2] #ypr[2], ypr[1], ypr[0]\n",
    "    elif quat==1:\n",
    "        return trans.translation.x, trans.translation.y, trans.translation.z, trans.rotation.x, trans.rotation.y, trans.rotation.z, trans.rotation.w\n",
    "    \n",
    "\n",
    "\n",
    "def Axis_angle_to_Quat(vector,angle):\n",
    "    x=vector[0]\n",
    "    y=vector[1]\n",
    "    z=vector[2]\n",
    "    qx = x * np.sin(angle/2)\n",
    "    qy = y * np.sin(angle/2)\n",
    "    qz = z * np.sin(angle/2)\n",
    "    qw = np.cos(angle/2)\n",
    "    \n",
    "    return qx,qy,qz,qw\n",
    "\n",
    "def Axis_angle_to_Euler(vector,angle):\n",
    "    x=vector[0]\n",
    "    y=vector[1]\n",
    "    z=vector[2]\n",
    "    s=np.sin(angle)\n",
    "    c=np.cos(angle)\n",
    "    t=1-c\n",
    "    \n",
    "    if ((x*y*t + z*s) > 0.998):  #north pole singularity detected\n",
    "        \n",
    "        heading = 2 * np.arctan2(x*np.sin(angle/2), np.cos(angle/2))\n",
    "        attitude = np.pi/2\n",
    "        bank = 0\n",
    "        return heading, attitude, bank\n",
    "    \n",
    "    elif ((x*y*t + z*s) < -0.998):\n",
    "        \n",
    "        heading = -2 * np.arctan2(x*np.sin(angle/2), np.cos(angle/2))\n",
    "        attitude = -np.pi/2\n",
    "        bank = 0\n",
    "        return heading, attitude, bank\n",
    "    \n",
    "    heading = np.arctan2(y * s- x * z * t , 1 - (y*y+ z*z ) * t)\n",
    "    attitude = np.arcsin(x * y * t + z * s)\n",
    "    bank = np.arctan2(x * s - y * z * t , 1 - (x*x + z*z) * t)\n",
    "    \n",
    "    return bank, heading, attitude\n",
    "\n",
    "\n",
    "def getRotation(v1):\n",
    "    if np.all(v1==[0., 0., 1.]): v1 = [0, 0.000001, 0.999999]\n",
    "    vec_x = [1.,0.,0.] \n",
    "    vec_y = [0.,1.,0.]\n",
    "    vec_z = [0.,0.,1.] #>>>>>>>>>>>>>>>>>>>>> change these to -1 to flip again\n",
    "    \n",
    "    vec1 = v1 / np.linalg.norm(v1)\n",
    "\n",
    "    #vector_x = np.cross(vec1, vec_x)/np.linalg.norm(np.cross(vec1, vec_x))\n",
    "    angle_x = (math.acos(np.dot(vec1, vec_x)))\n",
    "\n",
    "    #vector_y = np.cross(vec1, vec_y)/np.linalg.norm(np.cross(vec1, vec_y))\n",
    "    angle_y = (math.acos(np.dot(vec1, vec_y)))\n",
    "\n",
    "    vector_z = np.cross(vec1, vec_z)/np.linalg.norm(np.cross(vec1, vec_z))\n",
    "    angle_z = -(math.acos(np.dot(vec1, vec_z)))  #<<<Changed to adapt\n",
    "\n",
    "    #Rotation = filtered_Pt_Cloud.get_rotation_matrix_from_axis_angle(angle*vector) #alternative Open3D lib.\n",
    "    Rotation = R.from_rotvec(angle_z*vector_z)\n",
    "    \n",
    "    return Rotation, angle_x, angle_y, angle_z\n",
    "\n",
    "\n",
    "def Bounds_gen(minB, maxB, spacing):\n",
    "    ctr = 0\n",
    "    bounds = np.array([[0.,0.]])\n",
    "    CurrB = minB\n",
    "\n",
    "    while CurrB < maxB-0.5*spacing: #from left most to right most\n",
    "\n",
    "        LowerB = minB + ctr * spacing #if we shift the X or Y coordinates in multiples of spacing, we should get different lines.\n",
    "        if ctr == 0:\n",
    "            CurrB = LowerB + spacing/2 #lower + spacing/2 only for first condition\n",
    "        else:\n",
    "            CurrB = LowerB + spacing #lower + spacing\n",
    "        bounds = np.append(bounds,[[LowerB,CurrB]], axis=0)\n",
    "        #print(\"CurrB:\",CurrB,\"MaxB:\",maxB,\"diff:\",maxB-CurrB)\n",
    "        ctr+= 1\n",
    "    bounds = np.delete(bounds, 0, axis=0)\n",
    "    return bounds\n",
    "\n",
    "def cropped_PC(original_PC, spacing, X=0, idx = 0, centered = True, resample = True):\n",
    "    PC_BBOX = original_PC.get_axis_aligned_bounding_box()\n",
    "    minB_X = PC_BBOX.min_bound[0]\n",
    "    maxB_X = PC_BBOX.max_bound[0]\n",
    "    minB_Y = PC_BBOX.min_bound[1]\n",
    "    maxB_Y = PC_BBOX.max_bound[1]\n",
    "    minB_Z = PC_BBOX.min_bound[2]\n",
    "    maxB_Z = PC_BBOX.max_bound[2]\n",
    "    nor = np.array(original_PC.normals)\n",
    "    pts = np.array(original_PC.points)\n",
    "\n",
    "    Xs,Ys = get_XY_angles_from_PC(nor)\n",
    "    X_dev = np.std(Xs)\n",
    "    Y_dev = np.std(Ys)\n",
    "    print(\"X Standard Dev:\", X_dev, \"    Y Standard Dev:\", Y_dev)\n",
    "    \n",
    "    if(X > 1):\n",
    "        if(X==2): #Manual override at 2\n",
    "            X = 0  \n",
    "            print(\"Manual Override! taking curve around World Y-axis\",\"X=\", X)\n",
    "        elif(X==3):  #Manual override at 3\n",
    "            X = 1\n",
    "            print(\"Manual Override! taking curve around World X-axis\",\"X=\", X)\n",
    "        else: X = 1\n",
    "    else:\n",
    "        \n",
    "        if (X_dev <= Y_dev): \n",
    "            X = 1\n",
    "            print(\"Object's curve around World X-axis\",\"X=\", X)\n",
    "        elif (X_dev > Y_dev): \n",
    "            X = 0\n",
    "            print(\"Object's curve around World Y-axis\",\"X=\", X)\n",
    "\n",
    "    if X == 1: #X sided sweep crop\n",
    "        bounds = Bounds_gen(minB_X, maxB_X, spacing)\n",
    "        print(\"Total bounds:\",len(bounds))\n",
    "        if centered == True:\n",
    "            idx = int(np.floor(len(bounds)/2))\n",
    "        else:\n",
    "            if idx > len(bounds)-1:\n",
    "                print(\"Warning! Index value out of bounds, setting to max value\")\n",
    "                idx = -1\n",
    "        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(bounds[idx][0], minB_Y, minB_Z), max_bound=(bounds[idx][1], maxB_Y, maxB_Z))\n",
    "\n",
    "        result = original_PC.crop(bbox)\n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "        \n",
    "        if resample == True:\n",
    "            min_idx = np.argmin( np.std(res_pts, axis=0) )\n",
    "            res_pts[:, min_idx] = np.mean(res_pts,axis=0)[min_idx]  #replace values of axis (x,y or z)\n",
    "                                                                    #that has min std_dev by its mean.\n",
    "            result.points = o3d.utility.Vector3dVector(res_pts)\n",
    "            result = result.voxel_down_sample(voxel_size=spacing)\n",
    "        \n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "\n",
    "        ind = np.argsort(res_pts[:, 1]) #Sort Y coordinates from lowest to highest, X values are almost constant\n",
    "        res_pts = res_pts[ind]  #no longer required to change values to negative \n",
    "        res_nor = res_nor[ind]\n",
    "\n",
    "    elif X == 0: #Y sided sweep crop\n",
    "        bounds = Bounds_gen(minB_Y, maxB_Y, spacing)\n",
    "        print(\"Total bounds:\",len(bounds))\n",
    "        if centered == True:\n",
    "            idx = int(np.floor(len(bounds)/2))\n",
    "        else:\n",
    "            if idx > len(bounds)-1:\n",
    "                print(\"Warning! Index value out of bounds, setting to max value\")\n",
    "                idx = -1\n",
    "        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(minB_X, bounds[idx][0], minB_Z), max_bound=(maxB_X, bounds[idx][1], maxB_Z)) \n",
    "        print(bbox)\n",
    "\n",
    "        result = original_PC.crop(bbox)\n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "        \n",
    "        if resample == True:\n",
    "            min_idx = np.argmin( np.std(res_pts, axis=0) )\n",
    "            res_pts[:, min_idx] = np.mean(res_pts,axis=0)[min_idx]  #replace values of axis (x,y or z)\n",
    "                                                                    #that has min std_dev by its mean.\n",
    "            result.points = o3d.utility.Vector3dVector(res_pts)\n",
    "            result = result.voxel_down_sample(voxel_size=spacing)\n",
    "        \n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "\n",
    "        ind = np.argsort(res_pts[:, 0]) #Sort X coordinates from lowest to highest, Y values are almost constant\n",
    "        res_pts = res_pts[ind]\n",
    "        #res_pts[:,2] *=-1  #Change z values to negative\n",
    "        res_nor = res_nor[ind]\n",
    "\n",
    "    sorted_pointcloud = o3d.geometry.PointCloud()\n",
    "    sorted_pointcloud.points = o3d.utility.Vector3dVector(res_pts)\n",
    "    sorted_pointcloud.normals = o3d.utility.Vector3dVector(res_nor)\n",
    "    \n",
    "    return sorted_pointcloud\n",
    "\n",
    "\n",
    "def generate_coordinates(point_cloud):\n",
    "    Coordinates = np.array([[0.,0.,0.,0.,0.,0.,0.]])\n",
    "    world_Coordinates = np.array([[0.,0.,0.,0.,0.,0.,0.]])\n",
    "    point_cloud_pts = np.array(point_cloud.points)\n",
    "    point_cloud_nor = np.array(point_cloud.normals)\n",
    "    \n",
    "    for index in range (len(point_cloud_pts)):\n",
    "\n",
    "        rotat, angle_x, angle_y, angle_z = getRotation(-point_cloud_nor[index])  #<<<changed here pts -ve\n",
    "\n",
    "        a1 = rotat.as_matrix()[0][0]\n",
    "        a2 = rotat.as_matrix()[0][1]\n",
    "        a3 = rotat.as_matrix()[0][2]\n",
    "        b1 = rotat.as_matrix()[1][0]\n",
    "        b2 = rotat.as_matrix()[1][1]\n",
    "        b3 = rotat.as_matrix()[1][2]\n",
    "        c1 = rotat.as_matrix()[2][0]\n",
    "        c2 = rotat.as_matrix()[2][1]\n",
    "        c3 = rotat.as_matrix()[2][2]\n",
    "\n",
    "        KDL_original_plane_frame = PyKDL.Frame(PyKDL.Rotation(a1,a2,a3, b1,b2,b3, c1,c2,c3),\n",
    "                                               PyKDL.Vector(point_cloud_pts[index][0],\n",
    "                                                            point_cloud_pts[index][1],\n",
    "                                                             point_cloud_pts[index][2]))\n",
    "\n",
    "        KDL_flip_frame = PyKDL.Frame(PyKDL.Rotation.RPY(0, np.pi, 0), PyKDL.Vector(0,0,0)) #mirror plane frame to match camera frame\n",
    "        KDL_final_frame = KDL_original_plane_frame * KDL_flip_frame\n",
    "        KDL_trans = KDL_final_frame.p\n",
    "        KDL_ROT_quat = KDL_final_frame.M.GetQuaternion() \n",
    "\n",
    "        Coordinates = np.append(Coordinates,[[KDL_trans[0] ,KDL_trans[1],KDL_trans[2] ,KDL_ROT_quat[0] \n",
    "                                     ,KDL_ROT_quat[1] ,KDL_ROT_quat[2],KDL_ROT_quat[3]]],axis=0)\n",
    "\n",
    "        Publish_coordinates([[KDL_trans[0] ,KDL_trans[1],KDL_trans[2] ,KDL_ROT_quat[0] \n",
    "                            ,KDL_ROT_quat[1] ,KDL_ROT_quat[2],KDL_ROT_quat[3]]], \n",
    "                            'camera_depth_optical_frame', 'plane', static = True)\n",
    "\n",
    "        transform_plane = fetch_transform(tfbuffer,'world', 'plane_static_0',quat=1)\n",
    "\n",
    "        world_Coordinates = np.append(world_Coordinates,[[transform_plane[0] ,transform_plane[1]\n",
    "                                            ,transform_plane[2] ,transform_plane[3] ,transform_plane[4] \n",
    "                                             ,transform_plane[5] ,transform_plane[6]]],axis=0)\n",
    "\n",
    "    Coordinates = np.delete(Coordinates, 0, axis=0)\n",
    "    world_Coordinates2 = np.delete(world_Coordinates, 0, axis=0)\n",
    "    return Coordinates, world_Coordinates2\n",
    "\n",
    "def Publish_coordinates(Coordinates, parent_name, child_name, static = False): #Coordinates expects [[0,0,0,0,0,0,0],[1,1,1,1,1,1,1]] format\n",
    "\n",
    "    for index in range (len(Coordinates)):\n",
    "        \n",
    "        if static==True:\n",
    "            static_t.header.stamp = rospy.Time.now()\n",
    "            static_t.header.frame_id = parent_name #\"camera_depth_optical_frame\"\n",
    "            static_t.child_frame_id = child_name+\"_static_\"+str(index)\n",
    "            static_t.transform.translation.x = Coordinates[index][0]\n",
    "            static_t.transform.translation.y = Coordinates[index][1]\n",
    "            static_t.transform.translation.z = Coordinates[index][2]\n",
    "\n",
    "            #r_quat = tf.transformations.quaternion_from_euler(Roll,Pitch,Yaw)\n",
    "            static_t.transform.rotation.x = Coordinates[index][3]\n",
    "            static_t.transform.rotation.y = Coordinates[index][4]\n",
    "            static_t.transform.rotation.z = Coordinates[index][5]\n",
    "            static_t.transform.rotation.w = Coordinates[index][6]\n",
    "            static_br.sendTransform(static_t)\n",
    "        else:\n",
    "            t.header.frame_id = parent_name #\"camera_depth_optical_frame\"\n",
    "            t.child_frame_id = child_name+\"_\"+str(index)\n",
    "\n",
    "            t.header.stamp = rospy.Time.now()\n",
    "            t.transform.translation.x = Coordinates[index][0]\n",
    "            t.transform.translation.y = Coordinates[index][1]\n",
    "            t.transform.translation.z = Coordinates[index][2] \n",
    "\n",
    "            #r_quat = tf.transformations.quaternion_from_euler(Roll,Pitch,Yaw)\n",
    "            t.transform.rotation.x = Coordinates[index][3]\n",
    "            t.transform.rotation.y = Coordinates[index][4]\n",
    "            t.transform.rotation.z = Coordinates[index][5]\n",
    "            t.transform.rotation.w = Coordinates[index][6]\n",
    "            br.sendTransform(t)\n",
    "\n",
    "\n",
    "def Cluster_Point_Cloud(original_PC, eps=0.02, min_points=10):\n",
    "\n",
    "    labels = np.array(original_PC.cluster_dbscan(eps=eps, min_points=min_points))\n",
    "    uniques = np.unique(labels)\n",
    "    clouds = np.array(original_PC)\n",
    "    \n",
    "    if ((uniques[0] == -1) and (len(uniques) == 1)):\n",
    "        clouds = np.append(clouds,[original_PC])\n",
    "\n",
    "    else:\n",
    "        for i in range (len(uniques)):\n",
    "\n",
    "            if uniques[i] > -1:\n",
    "                idx = np.where(labels==uniques[i])[0]\n",
    "                cluster_pcd = original_PC.select_by_index(idx, invert=False)\n",
    "                clouds = np.append(clouds,[cluster_pcd])\n",
    "\n",
    "    clouds = np.delete(clouds, 0)\n",
    "    print(\"Number of clusters:\", len(clouds))\n",
    "    return clouds\n",
    "\n",
    "\n",
    "def go_to_coord_goal(move_group,coord):\n",
    "    pose_goal = geometry_msgs.msg.Pose()\n",
    "    pose_goal.position.x = coord[0]\n",
    "    pose_goal.position.y = coord[1]\n",
    "    pose_goal.position.z = coord[2]\n",
    "    pose_goal.orientation.x = coord[3]\n",
    "    pose_goal.orientation.y = coord[4]\n",
    "    pose_goal.orientation.z = coord[5]\n",
    "    pose_goal.orientation.w = coord[6]\n",
    "\n",
    "    move_group.set_pose_target(pose_goal)\n",
    "    \n",
    "    success = move_group.go(wait=True)\n",
    "    \n",
    "    move_group.stop()\n",
    "    move_group.clear_pose_targets()\n",
    "    current_pose = move_group.get_current_pose().pose\n",
    "\n",
    "def array_to_data(array):\n",
    "    im = PIL_img.fromarray(array)\n",
    "    output_buffer = BytesIO()\n",
    "    im.save(output_buffer, format=\"PNG\")\n",
    "    data = output_buffer.getvalue()\n",
    "    return data\n",
    "\n",
    "def fetch_cloud_image(pointCloud, RX=0, RY=0, RZ=0):\n",
    "    mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1,origin=[0, 0, 0])\n",
    "    mesh_pc= mesh.sample_points_uniformly(number_of_points=1000000, use_triangle_normal=False)\n",
    "    tmp_cloud = copy.deepcopy(pointCloud)  #To avoid overwriting the original point cloud\n",
    "    \n",
    "    tmp_cloud+=mesh_pc\n",
    "    tmp_Rot = pointCloud.get_rotation_matrix_from_xyz((np.radians(RX), np.radians(RY), np.radians((RZ))))\n",
    "    \n",
    "    tmp_cloud.rotate(tmp_Rot, center=(0, 0, 0))\n",
    "    \n",
    "    vis = o3d.visualization.Visualizer() \n",
    "    vis.create_window(visible=False, width=640, height=480) \n",
    "    vis.add_geometry(tmp_cloud) \n",
    "    vis.poll_events() \n",
    "    vis.update_renderer() \n",
    "    color = vis.capture_screen_float_buffer(True) \n",
    "    #time.sleep(5)\n",
    "    vis.destroy_window() \n",
    "    #color = np.asarray(color)\n",
    "    color = (255.0 * np.asarray(color)).astype(np.uint8)\n",
    "    color = array_to_data(color) #Format according to the GUI requirements\n",
    "    return color\n",
    "\n",
    "def Cluster_selection_gui(clouds):\n",
    "    dat=[]  #to hold cluster's images\n",
    "    pcd_combined = o3d.geometry.PointCloud()\n",
    "    \n",
    "    for current_cloud in clouds:\n",
    "            color = fetch_cloud_image(current_cloud, RX=120,RZ=180)  #different point clouds to be viewed, set view by RX,RY,RZ\n",
    "            dat.append(color)\n",
    "    result = subprocess.run([sys.executable,  \"Cluster_selection_gui.py\"],capture_output=True,text=True,check=True,shell=False, input=repr(dat))   \n",
    "    OP = literal_eval(result.stdout)\n",
    "    selected_PC = OP[0]\n",
    "    selected_mode = OP[1]\n",
    "    \n",
    "    for k in selected_PC:\n",
    "        pcd_combined += clouds[k]  #combine selected pointclouds \n",
    "    return pcd_combined, selected_mode\n",
    "\n",
    "#def Convert_message(string):\n",
    "#    return [int(s) for s in re.findall('[0-9]', string)]\n",
    "\n",
    "def get_XY_angles_from_PC(tst_downpcd_nor):\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    vec_x = [1.,0.,0.] \n",
    "    vec_y = [0.,1.,0.]\n",
    "    \n",
    "    for normal_vec in tst_downpcd_nor:\n",
    "        if np.all(normal_vec==[0., 0., 1.]): normal_vec = [0, 0.000001, 0.999999]\n",
    "        vec1 = normal_vec / np.linalg.norm(normal_vec)\n",
    "        angle_x = np.round(np.degrees((math.acos(np.dot(vec1, vec_x)))))\n",
    "        angle_y = np.round(np.degrees((math.acos(np.dot(vec1, vec_y)))))\n",
    "        Xs.append(angle_x)\n",
    "        Ys.append(angle_y)\n",
    "    Xs = np.array(Xs)\n",
    "    Ys = np.array(Ys)\n",
    "    return Xs, Ys\n",
    "\n",
    "\n",
    "def Generate_final_coordinates(world_coords, z_offset, eef_link, coord_skip=3):\n",
    "\n",
    "    Cam_target_final_coordinates=[]\n",
    "    EEF_target_final_coordinates=[]\n",
    "\n",
    "    for id_x in range(0,len(world_coords),coord_skip):  #Take every 4th point in coordinates\n",
    "        transform_world_plane = world_coords[id_x]\n",
    "        KDL_original_plane_frame = PyKDL.Frame(PyKDL.Rotation.Quaternion(transform_world_plane[3],\n",
    "                                                                         transform_world_plane[4], \n",
    "                                                                         transform_world_plane[5], \n",
    "                                                                         transform_world_plane[6]),\n",
    "                                               PyKDL.Vector(transform_world_plane[0], \n",
    "                                                            transform_world_plane[1], \n",
    "                                                            transform_world_plane[2]))\n",
    "\n",
    "        trans_x,trans_y,trans_z = KDL_original_plane_frame * PyKDL.Vector(0, 0, z_offset) #Add offset\n",
    "        KDL_original_plane_frame.p = PyKDL.Vector(trans_x,trans_y,trans_z) #update original plane frame to new location\n",
    "\n",
    "        KDL_flip_frame = PyKDL.Frame(PyKDL.Rotation.RPY(np.pi, 0.,np.pi), PyKDL.Vector(0, 0, 0)) #mirror plane frame to match camera frame\n",
    "\n",
    "        KDL_final_frame = KDL_original_plane_frame * KDL_flip_frame\n",
    "\n",
    "        KDL_trans = KDL_final_frame.p\n",
    "        KDL_ROT_quat = KDL_final_frame.M.GetQuaternion() \n",
    "\n",
    "        final_coordinates = [KDL_trans[0], KDL_trans[1], KDL_trans[2], KDL_ROT_quat[0], KDL_ROT_quat[1], \n",
    "                                                                       KDL_ROT_quat[2], KDL_ROT_quat[3]]\n",
    "        #print(final_coordinates)\n",
    "        #Publish_coordinates([final_coordinates], \"world\", 'Camera_Target', static = False)\n",
    "\n",
    "        Cam_target_final_coordinates.append(final_coordinates)\n",
    "\n",
    "        #Fetch transform between eef link (link8, or tcp) and optical depth cam\n",
    "        transform_eef_camera_depth = fetch_transform(tfbuffer,'camera_depth_optical_frame', eef_link, quat=1)\n",
    "\n",
    "        KDL_eef_cam_frame = PyKDL.Frame(PyKDL.Rotation.Quaternion(transform_eef_camera_depth[3],\n",
    "                                                                    transform_eef_camera_depth[4], \n",
    "                                                                    transform_eef_camera_depth[5],\n",
    "                                                                    transform_eef_camera_depth[6]),\n",
    "                                          PyKDL.Vector(transform_eef_camera_depth[0], \n",
    "                                                       transform_eef_camera_depth[1], \n",
    "                                                       transform_eef_camera_depth[2])) #eef-camera frame\n",
    "\n",
    "        #Offset original frame by z offset, flip the frame to match camera, multiply with eef-camera frame to \n",
    "        #replicate pose between camera and eef\n",
    "        KDL_final_frame = KDL_original_plane_frame * KDL_flip_frame * KDL_eef_cam_frame \n",
    "\n",
    "        KDL_trans = KDL_final_frame.p\n",
    "        KDL_ROT_quat = KDL_final_frame.M.GetQuaternion() \n",
    "\n",
    "        final_coordinates = [KDL_trans[0], KDL_trans[1], KDL_trans[2], KDL_ROT_quat[0], KDL_ROT_quat[1], \n",
    "                                                                       KDL_ROT_quat[2], KDL_ROT_quat[3]]\n",
    "        #Publish_coordinates([final_coordinates], \"world\", 'EEF_Target', static = True)\n",
    "        EEF_target_final_coordinates.append(final_coordinates)\n",
    "        \n",
    "    #Cam_target_final_coordinates.append(Cam_target_final_coordinates.pop(0))  #first element to last\n",
    "    #EEF_target_final_coordinates.append(EEF_target_final_coordinates.pop(0))  #first element to last\n",
    "    Cam_target_final_coordinates.pop(0)\n",
    "    EEF_target_final_coordinates.pop(0)\n",
    "    return Cam_target_final_coordinates, EEF_target_final_coordinates\n",
    "\n",
    "print(\"Loaded!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6be761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move_group.get_current_pose()\n",
    "\n",
    "initial_coordinates = [0.408,0.,0.834,0.9238709648606045,-0.3826807475252162,-0.003837162229149499,0.001778186465088758]\n",
    "\n",
    "initial_coordinates_vertical = [0.28701854121331966,-0.03260223409115588,0.44804421741139216,\n",
    "                       -0.6945874945768339,0.29634459105695476,-0.6174160637305418,0.2202850425613284]\n",
    "\n",
    "#Manipulator\n",
    "initial_coordinates_man = [0.4073103269798378,-1.4228208118709305e-05,0.7305284183578298,\n",
    "                       0.9999928346346129,-0.00048259177856538034,-0.003741879327895847,0.00031003822734821697]\n",
    "\n",
    "move_group.get_current_pose()\n",
    "\n",
    "go_to_coord_goal(move_group, initial_coordinates) #pass values to function to make robot move in cartesian space. \n",
    "print(\"Moving to initial position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d302cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 0.02 #spacing between each point in point cloud in meters (use this to approximate the shape of surface)\n",
    "offset_y = 0.13 #distance to crop in world's X axis away from robot, to avoid robot's shadow appearing in PC.\n",
    "offset_z = 0#0.15\n",
    "\n",
    "zoom_def = 1.34\n",
    "front_def = [ -0.38077889171563734, 0.78942164088651434, -0.48147783804018851 ]\n",
    "lookat_def = [ 0.16328584993371112, 0.10901604638723184, 0.39735867391549773 ]\n",
    "up_def = [ 0.17393478184042019, -0.45025906577602964, -0.87579304938588232 ]\n",
    "\n",
    "mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5,origin=[0, 0, 0])\n",
    "average_ptcloud = o3d.geometry.PointCloud()\n",
    "\n",
    "pts_cloud_tot = []\n",
    "pts_tot = []\n",
    "\n",
    "samples = 10\n",
    "\n",
    "def get_average_pt_cloud(offset_y,offset_z):\n",
    "    cam_robo_depth = abs(fetch_transform(tfbuffer,'camera_depth_optical_frame', 'panda_link0',quat=0)[2]) #get z value\n",
    "    color_frame, depth_frame = grab_frame()\n",
    "    downpcd = Generate_PointCloud(color_frame,depth_frame,from_depth=True, \n",
    "                                  depth_trunc = cam_robo_depth - 0.05, align=True) #Take from 4cm above ground\n",
    "    \n",
    "    #CROP TO FILTER OUT ROBOT'S SHADOW ADJUST OFFSET ACCORDINGLY\n",
    "    PC_BBOX = downpcd.get_axis_aligned_bounding_box()\n",
    "    minB_X = PC_BBOX.min_bound[0]\n",
    "    maxB_X = PC_BBOX.max_bound[0]\n",
    "    minB_Y = PC_BBOX.min_bound[1]\n",
    "    maxB_Y = PC_BBOX.max_bound[1]\n",
    "    minB_Z = PC_BBOX.min_bound[2]\n",
    "    maxB_Z = PC_BBOX.max_bound[2]\n",
    "\n",
    "    bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(minB_X, minB_Y+offset_y, minB_Z+offset_z), max_bound=(maxB_X, maxB_Y, maxB_Z)) \n",
    "    downpcd = downpcd.crop(bbox)\n",
    "    #o3d.visualization.draw_geometries([downpcd,mesh], point_show_normal=True, zoom=zoom_def, front=front_def, lookat=lookat_def, up=up_def)\n",
    "    return downpcd\n",
    "\n",
    "\n",
    "for i in range(samples):\n",
    "    new_pt = get_average_pt_cloud(offset_y,offset_z)\n",
    "    #average_ptcloud += new_pt\n",
    "    pts_cloud_tot.append(new_pt)\n",
    "    pts_tot.append(len(np.asarray(new_pt.points)))\n",
    "\n",
    "## We'll select only those point clouds that have a majority of similar \"number of points\". \n",
    "## this will prevent any outlier point clouds that have weird data in it (like less points \n",
    "## or more points than the group)\n",
    "\n",
    "bins = np.linspace(np.min(pts_tot), np.max(pts_tot), int(np.ceil(samples/3)))  #generate bins in range of number of pts in ptcloud\n",
    "digitized = np.digitize(pts_tot, bins) #put values in bins based on size of point clouds\n",
    "\n",
    "uni, count = np.unique(digitized,return_counts=True) #get the uniques and their counts\n",
    "unique_val = uni[np.argmax(count)]  #get the value from unique list where count was maximum\n",
    "idx_list = np.where(digitized == unique_val)[0]  #get all indexes from digitized list where it matches unique\n",
    "\n",
    "for idx in idx_list:\n",
    "    average_ptcloud += pts_cloud_tot[idx]\n",
    "\n",
    "#o3d.visualization.draw_geometries([average_ptcloud,mesh], point_show_normal=True, zoom=zoom_def, front=front_def, lookat=lookat_def, up=up_def)\n",
    "\n",
    "\n",
    "## Filter out hidden points\n",
    "diameter = np.linalg.norm(np.asarray(average_ptcloud.get_max_bound()) - np.asarray(average_ptcloud.get_min_bound()))\n",
    "cam = [0, 0, -diameter]\n",
    "radius = diameter * 100\n",
    "_, pt_map = average_ptcloud.hidden_point_removal(cam, radius) #Get all points that are visible from given view point\n",
    "average_ptcloud = average_ptcloud.select_by_index(pt_map)\n",
    "\n",
    "average_ptcloud = average_ptcloud.voxel_down_sample(voxel_size=spacing)\n",
    "average_ptcloud.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.06, max_nn=30)) #radius in meters\n",
    "average_ptcloud.orient_normals_towards_camera_location(camera_location=[0, 0, 0])\n",
    "\n",
    "Re = average_ptcloud.get_rotation_matrix_from_xyz((np.pi, 0 , 0))\n",
    "filtered_Pt_Cloud = average_ptcloud.rotate(Re, center=(0,0,0))\n",
    "o3d.visualization.draw_geometries([filtered_Pt_Cloud,mesh], point_show_normal=True, zoom=zoom_def, front=front_def, lookat=lookat_def, up=up_def)\n",
    "\n",
    "\n",
    "nor = np.array(filtered_Pt_Cloud.normals)\n",
    "pts = np.array(filtered_Pt_Cloud.points)\n",
    "\n",
    "distance,index = spatial.KDTree(pts).query( filtered_Pt_Cloud.get_center() ) #find coordinates \n",
    "                                                                             #that are closest to the center \n",
    "\n",
    "print(distance)\n",
    "print(index)\n",
    "print()\n",
    "print(\"Center Coordinates(ground truth):\",filtered_Pt_Cloud.get_center())\n",
    "print(\"Points Coordinates(estimated center point):\",pts[index])\n",
    "print(\"Normal Coordinates(Normal of estimated center point):\",nor[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b51d7",
   "metadata": {},
   "source": [
    "### Clustering of points in Point cloud\n",
    "ref: http://www.open3d.org/docs/release/tutorial/geometry/pointcloud.html#DBSCAN-clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e32e8",
   "metadata": {},
   "source": [
    "### Crop point cloud to get a points in a single line in X or Y axis, \n",
    "*Use idx to give offset else use centered to get the points from center of the object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds = Cluster_Point_Cloud(filtered_Pt_Cloud, eps=0.05, min_points=10)\n",
    "\n",
    "result,selected_mode = Cluster_selection_gui(clouds) #Will present a GUI to select all relevant pointclouds\n",
    "#o3d.visualization.draw_geometries([result, mesh], point_show_normal=True, zoom=zoom_def, front=front_def, lookat=lookat_def, up=up_def)\n",
    "\n",
    "result = cropped_PC(result, spacing, X=selected_mode, idx = 0, centered = True, resample = True)\n",
    "o3d.visualization.draw_geometries([result,mesh], point_show_normal=True, zoom=zoom_def, front=front_def, lookat=lookat_def, up=up_def)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf797ad",
   "metadata": {},
   "source": [
    "### This will generate and boradcast all points in pointcloud to tf2 or tf2_static topic, visualize using Rviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84aed30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cam_coords, world_coords = generate_coordinates(result)\n",
    "#cam_coords, world_coords = generate_coordinates(clouds[0])\n",
    "print(\"Total coordinates:\",len(world_coords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f63a8",
   "metadata": {},
   "source": [
    "## Publish Targets for Preview"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3896ca5",
   "metadata": {},
   "source": [
    "#Publish_coordinates(world_coords,'world','plane', static = False)\n",
    "for i in range (len(world_coords)):\n",
    "    Publish_coordinates([world_coords[i]],'world','plane', static = False)\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cbcd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_to_coord_goal(move_group, initial_coordinates_man) #pass values to function to make robot move in cartesian space. \n",
    "print(\"Moving to initial position\")\n",
    "\n",
    "\n",
    "z_offset = 0.1 #how far camera should be from object (make sure to take eef into account)\n",
    "eef_link = move_group.get_end_effector_link()  # get current eef link either link8 if panda_arm selected \n",
    "                                               # or hand_tcp if panda_manipulator is selected.\n",
    "cam_tgt, eef_tgt = Generate_final_coordinates(world_coords, z_offset, eef_link, coord_skip=1) #Take every 4th point in coordinates\n",
    "\n",
    "Publish_coordinates(cam_tgt, \"world\", 'Camera_Target', static = False)\n",
    "#Publish_coordinates(eef_tgt, \"world\", 'EEF_Target', static = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a1811",
   "metadata": {},
   "source": [
    "## Robot Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24843072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_offset = 0.3 #how far camera should be from object (make sure to take eef into account)\n",
    "\n",
    "eef_link = move_group.get_end_effector_link()  # get current eef link either link8 if panda_arm selected \n",
    "                                               # or hand_tcp if panda_manipulator is selected.\n",
    "\n",
    "cam_tgt, eef_tgt = Generate_final_coordinates(world_coords, z_offset, eef_link, coord_skip=3) #Take every 4th point in coordinates\n",
    "\n",
    "for id_x in range(0,len(eef_tgt)):  \n",
    "\n",
    "    Publish_coordinates([cam_tgt[id_x]], \"world\", 'Camera_Target', static = False)   \n",
    "    Publish_coordinates([eef_tgt[id_x]], \"world\", 'EEF_Target', static = False)\n",
    "\n",
    "    ## Move Robot to TARGET\n",
    "    go_to_coord_goal(move_group, eef_tgt[id_x]) #pass values to function to make robot move in cartesian space. \n",
    "    print(\"Moving to Target:\",id_x+1)\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9587404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ea0e864f",
   "metadata": {},
   "source": [
    "print(\"============ Printing robot current Joint values\") #O/P: current joint values of the end-effector\n",
    "#print(move_group.get_current_joint_values())\n",
    "print(\"In Degrees: \", np.degrees( move_group.get_current_joint_values() ) ) #O/P joint angles in degrees\n",
    "print(\"\")\n",
    "\n",
    "print(\"============ Printing robot current pose\") #O/P will be XYZ and xyz w  positions and orientations of the robot.\n",
    "print(move_group.get_current_pose())\n",
    "print(\"\")\n",
    "\n",
    "print(\"============ Printing robot current RPY\") #O/P wil be orientation of end-effector in RPY (Radians)\n",
    "#print(move_group.get_current_rpy())\n",
    "print(\"RPY In Degrees: \", np.degrees( move_group.get_current_rpy() ) )\n",
    "\n",
    "eef_link = move_group.get_end_effector_link()\n",
    "print(\"============ End effector link: %s\" % eef_link) #O/P: ============ End effector link: panda_link8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
