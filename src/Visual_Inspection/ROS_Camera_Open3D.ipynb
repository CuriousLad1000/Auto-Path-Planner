{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b930f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import rospy\n",
    "\n",
    "import pyrealsense2\n",
    "from realsense_depth import *\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import open3d as o3d\n",
    "from open3d_ros_helper import open3d_ros_helper as orh\n",
    "from scipy import spatial\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import math\n",
    "import tf2_ros, tf\n",
    "import geometry_msgs.msg\n",
    "import PyKDL\n",
    "import time\n",
    "\n",
    "image = None\n",
    "\n",
    "rospy.init_node(\"my_pic\", anonymous=True)\n",
    "\n",
    "tfbuffer = tf2_ros.Buffer()\n",
    "listener = tf2_ros.TransformListener(tfbuffer)\n",
    "br = tf2_ros.TransformBroadcaster()\n",
    "t = geometry_msgs.msg.TransformStamped()\n",
    "\n",
    "bridge = CvBridge()\n",
    "loop_rate = rospy.Rate(0.5) # Node cycle rate (in Hz).\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a31f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_frame():\n",
    "    \n",
    "    frame_color=rospy.wait_for_message('/camera/color/image_raw', Image, timeout=None) #wait_for_message(topic, topic_type, timeout=None): \n",
    "    cv_image_color = bridge.imgmsg_to_cv2(frame_color, desired_encoding='rgb8')\n",
    "    \n",
    "    frame_depth = rospy.wait_for_message('/camera/depth/image_raw', Image, timeout=None) #wait_for_message(topic, topic_type, timeout=None): \n",
    "    cv_image_depth = bridge.imgmsg_to_cv2(frame_depth)\n",
    "    \n",
    "    return cv_image_color, cv_image_depth\n",
    "\n",
    "def visualize_rgbd(rgbd_image):\n",
    "    print(rgbd_image)\n",
    "    #o3d.visualization.draw_geometries([rgbd_image])\n",
    "    \n",
    "    #intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "    #intrinsic.intrinsic_matrix =  [[462.1379699707031, 0.0, 320.0], [0.0, 462.1379699707031, 240.0], [0.0, 0.0, 1.0]]\n",
    "    #intrinsic.intrinsic_matrix =  [[347.99755859375, 0.0, 320.0], [0.0, 347.99755859375, 240.0], [0.0, 0.0, 1.0]]\n",
    "    #intrinsic.intrinsic_matrix =  [[602.71783447, 0.0, 313.06835938], [0.0, 601.61364746, 230.37461853], [0.0, 0.0, 1.0]]\n",
    "    \n",
    "    #w = 640\n",
    "    #h = 480\n",
    "    #fx = 602.71783447\n",
    "    #fy = 601.61364746\n",
    "    #cx = 313.06835938\n",
    "    #cy = 230.37461853\n",
    "    \n",
    "    #Color frame ROS camera\n",
    "    #w = 640\n",
    "    #h = 480\n",
    "    #fx = 462.1379699707031\n",
    "    #fy = 462.1379699707031\n",
    "    #cx = 320.0\n",
    "    #cy = 240.0\n",
    "    \n",
    "    #Depth frame ROS camera\n",
    "    w = 640\n",
    "    h = 480\n",
    "    fx = 347.99755859375\n",
    "    fy = 347.99755859375\n",
    "    cx = 320.0\n",
    "    cy = 240.0    \n",
    "    \n",
    "    \n",
    "    intrinsic = o3d.camera.PinholeCameraIntrinsic(w, h, fx,fy, cx, cy)\n",
    "    intrinsic.intrinsic_matrix = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]\n",
    "    \n",
    "    cam = o3d.camera.PinholeCameraParameters()\n",
    "    cam.intrinsic = intrinsic\n",
    "    \n",
    "    #cam.extrinsic = np.array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 1.]])\n",
    "    #pcd = o3d.geometry.create_point_cloud_from_rgbd_image(rgbd_image, cam.intrinsic, cam.extrinsic)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, cam.intrinsic)\n",
    "    #pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image,intrinsic)\n",
    "    \n",
    "    pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]) # Flip it, otherwise the pointcloud will be upside down.\n",
    "    \n",
    "    #o3d.visualization.draw_geometries([pcd])\n",
    "    \n",
    "    return pcd\n",
    "    \n",
    "    \n",
    "def tst_dataset(color_frame,depth_frame):\n",
    "    color_raw = o3d.geometry.Image(np.asarray(color_frame))\n",
    "    depth_raw = o3d.geometry.Image(np.asarray(depth_frame.astype(np.uint16)) )\n",
    "    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(color_raw, depth_raw, convert_rgb_to_intensity=False)\n",
    "    pcd = visualize_rgbd(rgbd_image)\n",
    "    return pcd\n",
    "\n",
    "\n",
    "def box_pos(x_coord, y_coord, width, height, centered=0):\n",
    "    if centered == 0:\n",
    "        start_point = (x_coord, y_coord) # represents the top left corner of rectangle\n",
    "        end_point = (x_coord+width-1, y_coord+height-1)  # represents the bottom right corner of rectangle\n",
    "    elif centered == 1:\n",
    "        new_x = x_coord - (np.floor(width/2)-1).astype(int)\n",
    "        new_y = y_coord - (np.floor(height/2)-1).astype(int)\n",
    "        start_point = (new_x, new_y) \n",
    "        end_point = (new_x+width, new_y+height)\n",
    "    return start_point, end_point\n",
    "\n",
    "\n",
    "def fetch_transform(tfbuffer,frame1,frame2,quat=0):\n",
    "    flag = 0\n",
    "    while flag==0:\n",
    "        try:\n",
    "            trans = tfbuffer.lookup_transform(frame1, frame2, rospy.Time(),rospy.Duration(8.0))\n",
    "            #print (trans)\n",
    "            trans = trans.transform  #save translation and rotation\n",
    "            #rot = PyKDL.Rotation.Quaternion(* [ trans.rotation.x,trans.rotation.y,trans.rotation.z,trans.rotation.w] )\n",
    "            #ypr = [ i  / np.pi * 180 for i in rot.GetEulerZYX() ]\n",
    "            #print(ypr[2],ypr[1],ypr[0])\n",
    "            \n",
    "            rot = R.from_quat([ trans.rotation.x,trans.rotation.y,trans.rotation.z,trans.rotation.w]) #creates rotation matrix\n",
    "            rpy = rot.as_euler('XYZ',degrees=True) #extrinsic\n",
    "            break\n",
    "        except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException) as e:\n",
    "            #print (\"Fail\", e)\n",
    "            continue\n",
    "    if quat==0:\n",
    "        return trans.translation.x, trans.translation.y, trans.translation.z, rpy[0],rpy[1],rpy[2] #ypr[2], ypr[1], ypr[0]\n",
    "    elif quat==1:\n",
    "        return trans.translation.x, trans.translation.y, trans.translation.z, trans.rotation.x, trans.rotation.y, trans.rotation.z, trans.rotation.w\n",
    "    \n",
    "\n",
    "\n",
    "def Axis_angle_to_Quat(vector,angle):\n",
    "    x=vector[0]\n",
    "    y=vector[1]\n",
    "    z=vector[2]\n",
    "    qx = x * np.sin(angle/2)\n",
    "    qy = y * np.sin(angle/2)\n",
    "    qz = z * np.sin(angle/2)\n",
    "    qw = np.cos(angle/2)\n",
    "    \n",
    "    return qx,qy,qz,qw\n",
    "\n",
    "def Axis_angle_to_Euler(vector,angle):\n",
    "    x=vector[0]\n",
    "    y=vector[1]\n",
    "    z=vector[2]\n",
    "    s=np.sin(angle)\n",
    "    c=np.cos(angle)\n",
    "    t=1-c\n",
    "    \n",
    "    if ((x*y*t + z*s) > 0.998):  #north pole singularity detected\n",
    "        \n",
    "        heading = 2 * np.arctan2(x*np.sin(angle/2), np.cos(angle/2))\n",
    "        attitude = np.pi/2\n",
    "        bank = 0\n",
    "        return heading, attitude, bank\n",
    "    \n",
    "    elif ((x*y*t + z*s) < -0.998):\n",
    "        \n",
    "        heading = -2 * np.arctan2(x*np.sin(angle/2), np.cos(angle/2))\n",
    "        attitude = -np.pi/2\n",
    "        bank = 0\n",
    "        return heading, attitude, bank\n",
    "    \n",
    "    heading = np.arctan2(y * s- x * z * t , 1 - (y*y+ z*z ) * t)\n",
    "    attitude = np.arcsin(x * y * t + z * s)\n",
    "    bank = np.arctan2(x * s - y * z * t , 1 - (x*x + z*z) * t)\n",
    "    \n",
    "    return bank, heading, attitude\n",
    "\n",
    "\n",
    "def getRotation2(v1):\n",
    "    if np.all(v1==[0., 0., 1.]): v1 = [0, 0.000001, 0.999999]\n",
    "    vec_x = [1.,0.,0.] \n",
    "    vec_y = [0.,1.,0.]\n",
    "    vec_z = [0.,0.,1.] #>>>>>>>>>>>>>>>>>>>>> change these to -1 to flip again\n",
    "    \n",
    "    vec1 = v1 / np.linalg.norm(v1)\n",
    "\n",
    "    #vector_x = np.cross(vec1, vec_x)/np.linalg.norm(np.cross(vec1, vec_x))\n",
    "    angle_x = (math.acos(np.dot(vec1, vec_x)))\n",
    "\n",
    "    #vector_y = np.cross(vec1, vec_y)/np.linalg.norm(np.cross(vec1, vec_y))\n",
    "    angle_y = (math.acos(np.dot(vec1, vec_y)))\n",
    "\n",
    "    vector_z = np.cross(vec1, vec_z)/np.linalg.norm(np.cross(vec1, vec_z))\n",
    "    angle_z = (math.acos(np.dot(vec1, vec_z)))\n",
    "\n",
    "    #Rotation = filtered_Pt_Cloud.get_rotation_matrix_from_axis_angle(angle*vector) #alternative Open3D lib.\n",
    "    Rotation = R.from_rotvec(angle_z*vector_z)\n",
    "    \n",
    "    return Rotation, angle_x, angle_y, angle_z\n",
    "\n",
    "\n",
    "def Bounds_gen(minB, maxB, spacing):\n",
    "    UpperB = 0\n",
    "    ctr = 0\n",
    "    bounds = np.array([[0.,0.]])\n",
    "    while UpperB < maxB: #from left most to right most\n",
    "\n",
    "        LowerB = minB + ctr * spacing #if we shift the X or Y coordinates in multiples of spacing, we should get different lines.\n",
    "        if ctr == 0:\n",
    "            UpperB = LowerB + spacing/2 #lower + spacing/2 only for first condition\n",
    "        else:\n",
    "            UpperB = LowerB + spacing #lower + spacing\n",
    "        bounds = np.append(bounds,[[LowerB,UpperB]], axis=0)\n",
    "        ctr+= 1\n",
    "    bounds = np.delete(bounds, 0, axis=0)\n",
    "    return bounds\n",
    "\n",
    "def cropped_PC(original_PC, spacing, X=0, idx = 0, centered = True):\n",
    "    PC_BBOX = original_PC.get_axis_aligned_bounding_box()\n",
    "    minB_X = PC_BBOX.min_bound[0]\n",
    "    maxB_X = PC_BBOX.max_bound[0]\n",
    "    minB_Y = PC_BBOX.min_bound[1]\n",
    "    maxB_Y = PC_BBOX.max_bound[1]\n",
    "    minB_Z = PC_BBOX.min_bound[2]\n",
    "    maxB_Z = PC_BBOX.max_bound[2]\n",
    "    nor = np.array(original_PC.normals)\n",
    "    pts = np.array(original_PC.points)\n",
    "    distance,index = spatial.KDTree(pts).query( original_PC.get_center() )\n",
    "    \n",
    "    ro, angle_x, angle_y, angle_z = getRotation2(nor[index])\n",
    "    angle_x = np.round(np.degrees(angle_x))\n",
    "    angle_y = np.round(np.degrees(angle_y))\n",
    "\n",
    "    if ((angle_x == 90) and (angle_y != 90) and (abs(angle_x-angle_y)>=2)):\n",
    "        X = 1\n",
    "    elif ((angle_y == 90) and (angle_x != 90) and (abs(angle_x-angle_y)>=2)):\n",
    "        X = 0\n",
    "    if X == 1: #X sided sweep crop\n",
    "        bounds = Bounds_gen(minB_X, maxB_X, spacing)\n",
    "        print(\"Total bounds:\",len(bounds))\n",
    "        if centered == True:\n",
    "            idx = int(np.floor(len(bounds)/2))\n",
    "        else:\n",
    "            if idx > len(bounds)-1:\n",
    "                print(\"Warning! Index value out of bounds, setting to max value\")\n",
    "                idx = -1\n",
    "        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(bounds[idx][0], minB_Y, minB_Z), max_bound=(bounds[idx][1], maxB_Y, maxB_Z))\n",
    "\n",
    "        result = original_PC.crop(bbox)\n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "\n",
    "        ind = np.argsort(res_pts[:, 1]) #Sort Y coordinates from lowest to highest, X values are almost constant\n",
    "        res_pts = -res_pts[ind]  #Change all XYZ values to negative \n",
    "        res_nor = res_nor[ind]\n",
    "\n",
    "    elif X == 0: #Y sided sweep crop\n",
    "        bounds = Bounds_gen(minB_Y, maxB_Y, spacing)\n",
    "        print(\"Total bounds:\",len(bounds))\n",
    "        if centered == True:\n",
    "            idx = int(np.floor(len(bounds)/2))\n",
    "        else:\n",
    "            if idx > len(bounds)-1:\n",
    "                print(\"Warning! Index value out of bounds, setting to max value\")\n",
    "                idx = -1\n",
    "        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(minB_X, bounds[idx][0], minB_Z), max_bound=(maxB_X, bounds[idx][1], maxB_Z)) \n",
    "        print(bbox)\n",
    "\n",
    "        result = original_PC.crop(bbox)\n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "\n",
    "        ind = np.argsort(res_pts[:, 0]) #Sort X coordinates from lowest to highest, Y values are almost constant\n",
    "        res_pts = res_pts[ind]\n",
    "        res_pts[:,2] *=-1  #Change z values to negative\n",
    "        res_nor = res_nor[ind]\n",
    "\n",
    "    sorted_pointcloud = o3d.geometry.PointCloud()\n",
    "    sorted_pointcloud.points = o3d.utility.Vector3dVector(res_pts)\n",
    "    sorted_pointcloud.normals = o3d.utility.Vector3dVector(res_nor)\n",
    "    return sorted_pointcloud\n",
    "\n",
    "\n",
    "def Publish_coordinates(point_cloud):\n",
    "    point_cloud_pts = np.array(point_cloud.points)\n",
    "    point_cloud_nor = np.array(point_cloud.normals)\n",
    "    \n",
    "    for index in range (len(point_cloud_pts)):\n",
    "\n",
    "        rotat, angle_x, angle_y, angle_z = getRotation2(point_cloud_nor[index])\n",
    "        flip_frame = R.from_euler('xyz', [0., 180, 0.], degrees=True) \n",
    "        #flip_frame2 = R.from_euler('xyz', [0., 0., 180.], degrees=True) #Use if-else to switch between cases\n",
    "        #rotat = flip_frame*rotat*flip_frame2  #Use if-else to switch between cases\n",
    "\n",
    "        rotat = flip_frame*rotat  #to match correct normal/plane pose (flip later to send to robot)\n",
    "        r_quat = rotat.as_quat()            #x,y,z,w format\n",
    "\n",
    "        t.header.frame_id = \"camera_depth_optical_frame\"\n",
    "        t.child_frame_id = \"plane_\"+str(index)\n",
    "\n",
    "        t.header.stamp = rospy.Time.now()\n",
    "        t.transform.translation.x = point_cloud_pts[index][0]\n",
    "        t.transform.translation.y = point_cloud_pts[index][1]\n",
    "        t.transform.translation.z = point_cloud_pts[index][2] \n",
    "\n",
    "        #r_quat = tf.transformations.quaternion_from_euler(Roll,Pitch,Yaw)\n",
    "\n",
    "        t.transform.rotation.x = r_quat[0]\n",
    "        t.transform.rotation.y = r_quat[1]\n",
    "        t.transform.rotation.z = r_quat[2]\n",
    "        t.transform.rotation.w = r_quat[3]\n",
    "\n",
    "        br.sendTransform(t)\n",
    "\n",
    "\n",
    "def Cluster_Point_Cloud(original_PC, eps=0.02, min_points=10):\n",
    "\n",
    "    labels = np.array(original_PC.cluster_dbscan(eps=eps, min_points=min_points))\n",
    "    uniques = np.unique(labels)\n",
    "    clouds = np.array(original_PC)\n",
    "    \n",
    "    if ((uniques[0] == -1) and (len(uniques) == 1)):\n",
    "        clouds = np.append(clouds,[original_PC])\n",
    "\n",
    "    else:\n",
    "        for i in range (len(uniques)):\n",
    "\n",
    "            if uniques[i] > -1:\n",
    "                idx = np.where(labels==uniques[i])[0]\n",
    "                cluster_pcd = original_PC.select_by_index(idx, invert=False)\n",
    "                clouds = np.append(clouds,[cluster_pcd])\n",
    "\n",
    "    clouds = np.delete(clouds, 0)\n",
    "    print(\"Number of clusters:\", len(clouds))\n",
    "    return clouds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6d302cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color frame: (480, 640, 3)  Depth frame: (480, 640)\n",
      "Center of image: 260 mm\n",
      "RGBDImage of size \n",
      "Color image : 640x480, with 3 channels.\n",
      "Depth image : 640x480, with 1 channels.\n",
      "Use numpy.asarray to access buffer data.\n",
      "Recompute the normal of the downsampled point cloud\n",
      "Align normals towards camera\n",
      "PointCloud with 705 points.\n",
      "\n",
      "0.03448057648234389\n",
      "382\n",
      "\n",
      "Center Coordinates(ground truth): [ 0.01353568 -0.02319428 -0.29318065]\n",
      "Points Coordinates(estimated center point): [ 0.005969   -0.02724809 -0.25978571]\n",
      "Normal Coordinates(Normal of estimated center point): [-0.13709624 -0.00854948  0.99052084]\n"
     ]
    }
   ],
   "source": [
    "color_frame, depth_frame = grab_frame()\n",
    "print(\"color frame:\",color_frame.shape, \" Depth frame:\",depth_frame.shape)\n",
    "\n",
    "#Check center pixel distance\n",
    "\n",
    "point = (320, 240)\n",
    "\n",
    "spacing = 0.01 #spacing between each point in point cloud in meters (use this to approximate the shape of surface)\n",
    "offset = 0.13 #distance to crop in world's X axis away from robot, to avoid robot's shadow appearing in PC.\n",
    "\n",
    "mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1,origin=[0, 0, 0])\n",
    "\n",
    "distance = depth_frame[point[1], point[0]]\n",
    "print(\"Center of image:\", distance, \"mm\")\n",
    "\n",
    "pt_cloud = tst_dataset(color_frame, depth_frame)\n",
    "\n",
    "downpcd = pt_cloud.voxel_down_sample(voxel_size=spacing)\n",
    "\n",
    "print(\"Recompute the normal of the downsampled point cloud\")\n",
    "downpcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.06, max_nn=30)) #radius in meters\n",
    "print(\"Align normals towards camera\")\n",
    "downpcd.orient_normals_towards_camera_location(camera_location=[0, 0, 0])\n",
    "\n",
    "\n",
    "#o3d.visualization.draw_geometries([downpcd,mesh], point_show_normal=True)\n",
    "\n",
    "#v2_camera = np.array(downpcd.normals)\n",
    "v2_camera_pts = np.array(downpcd.points)\n",
    "\n",
    "\n",
    "#idx = np.where(abs(v2_camera_pts[:,2]) < (distance/1000)+0.01)[0] #fetch all indexes of values less than distance of center of img 0.3 in column 3 of rec\n",
    "idx = np.where(abs(v2_camera_pts[:,2]) < abs(np.min(v2_camera_pts[:,2]))-0.06)[0] #fetch all indexes of values less than distance of center of img 0.3 in column 3 of rec\n",
    "\n",
    "#print(idx)\n",
    "filtered_Pt_Cloud = downpcd.select_by_index(idx, invert=False)\n",
    "\n",
    "\n",
    "#CROP TO FILTER OUT ROBOT'S SHADOW ADJUST OFFSET ACCORDINGLY\n",
    "PC_BBOX = filtered_Pt_Cloud.get_axis_aligned_bounding_box()\n",
    "print(PC_BBOX)\n",
    "minB_X = PC_BBOX.min_bound[0]\n",
    "maxB_X = PC_BBOX.max_bound[0]\n",
    "minB_Y = PC_BBOX.min_bound[1]\n",
    "maxB_Y = PC_BBOX.max_bound[1]\n",
    "minB_Z = PC_BBOX.min_bound[2]\n",
    "maxB_Z = PC_BBOX.max_bound[2]\n",
    "bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(minB_X, minB_Y+offset, minB_Z+offset), max_bound=(maxB_X, maxB_Y, maxB_Z)) \n",
    "filtered_Pt_Cloud = filtered_Pt_Cloud.crop(bbox)\n",
    "\n",
    "\n",
    "\n",
    "print(filtered_Pt_Cloud)\n",
    "o3d.visualization.draw_geometries([filtered_Pt_Cloud,mesh], point_show_normal=True)\n",
    "nor = np.array(filtered_Pt_Cloud.normals)\n",
    "pts = np.array(filtered_Pt_Cloud.points)\n",
    "\n",
    "print()\n",
    "distance,index = spatial.KDTree(pts).query( filtered_Pt_Cloud.get_center() ) #find coordinates \n",
    "                                                                             #that are closest to the center \n",
    "\n",
    "print(distance)\n",
    "print(index)\n",
    "print()\n",
    "print(\"Center Coordinates(ground truth):\",filtered_Pt_Cloud.get_center())\n",
    "print(\"Points Coordinates(estimated center point):\",pts[index])\n",
    "print(\"Normal Coordinates(Normal of estimated center point):\",nor[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0affb1",
   "metadata": {},
   "source": [
    "### Create Coordinate pointer mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e5aae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mesh with coordinate\n",
    "mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1,origin=[0, 0, 0])\n",
    "o3d.visualization.draw_geometries([filtered_Pt_Cloud,mesh], point_show_normal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e32e8",
   "metadata": {},
   "source": [
    "### Crop point cloud to get a points in a single line in X or Y axis, \n",
    "*Use idx to give offset else use centered to get the points from center of the object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d343540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bounds: 47\n",
      "AxisAlignedBoundingBox: min: (-0.0817678, -0.125521, -0.520941), max: (0.113938, -0.115521, -0.351725)\n"
     ]
    }
   ],
   "source": [
    "Point_Cloud_Number = 0\n",
    "\n",
    "clouds2 = Cluster_Point_Cloud(downpcd)\n",
    "result = cropped_PC(clouds2[1], spacing, X=0, idx = 0, centered = True)\n",
    "\n",
    "#result = cropped_PC(filtered_Pt_Cloud, spacing, X=0, idx = 0, centered = True)\n",
    "o3d.visualization.draw_geometries([result, mesh], point_show_normal=True)\n",
    "\n",
    "#clouds = Cluster_Point_Cloud(result)\n",
    "\n",
    "#res_pts = np.array(clouds[Point_Cloud_Number].points)\n",
    "#res_nor = np.array(clouds[Point_Cloud_Number].normals)\n",
    "\n",
    "\n",
    "res_pts = np.array(result.points)\n",
    "res_nor = np.array(result.normals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9edfe",
   "metadata": {},
   "source": [
    "### Clustering of points in Point cloud\n",
    "ref: http://www.open3d.org/docs/release/tutorial/geometry/pointcloud.html#DBSCAN-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_label = labels.max()\n",
    "#print(f\"point cloud has {max_label + 1} clusters\")\n",
    "#colors = plt.get_cmap(\"tab20\")(labels / (max_label if max_label > 0 else 1))\n",
    "#colors[labels < 0] = 0\n",
    "#pcd.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "#o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "#o3d.visualization.draw_geometries([pcd, mesh], point_show_normal=True)\n",
    "\n",
    "clouds2 = Cluster_Point_Cloud(downpcd)\n",
    "o3d.visualization.draw_geometries([clouds2[1], mesh], point_show_normal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf797ad",
   "metadata": {},
   "source": [
    "### This will boradcast all points in pointcloud to tf2 topic, visualize using Rviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b7e697f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Publish_coordinates(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da656b43",
   "metadata": {},
   "source": [
    "### Snip to test and visualize the tf2 frame order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "46918e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range (len(res_pts)):\n",
    "\n",
    "    rotat, angle_x, angle_y, angle_z = getRotation2(res_nor[index])\n",
    "\n",
    "    flip_frame = R.from_euler('xyz', [0., 180, 0.], degrees=True) \n",
    "    #flip_frame2 = R.from_euler('xyz', [0., 0., 180.], degrees=True) #Use if-else to switch between cases\n",
    "    #rotat = flip_frame*rotat*flip_frame2  #Use if-else to switch between cases\n",
    "\n",
    "    rotat = flip_frame*rotat  #to match correct normal/plane pose (flip later to send to robot)\n",
    "\n",
    "    r = rotat.as_euler(\"xyz\", degrees=True)\n",
    "\n",
    "    r_quat = rotat.as_quat()            #x,y,z,w format\n",
    "\n",
    "    print(\"X axis and normal angle in degrees\",np.degrees(angle_x))\n",
    "    print()\n",
    "    print(\"Y axis and normal angle in degrees\",np.degrees(angle_y))\n",
    "    print()\n",
    "    print(\"Z axis and normal angle in degrees\",np.degrees(angle_z))\n",
    "    print()\n",
    "\n",
    "    print(\"Rotation wrt camera:\",\"Roll:\", np.round(r[0],3), \"Pitch:\", np.round(r[1],3), \"Yaw:\", np.round(r[2],3))\n",
    "    #print(\"In Quaternion form xyzw:\",r_quat)\n",
    "    print()\n",
    "    print(\"Coordinates wrt camera:\",res_pts[index])\n",
    "\n",
    "    t.header.frame_id = \"camera_depth_optical_frame\"\n",
    "    t.child_frame_id = \"plane\"\n",
    "\n",
    "    t.header.stamp = rospy.Time.now()\n",
    "    t.transform.translation.x = res_pts[index][0]\n",
    "    t.transform.translation.y = res_pts[index][1]\n",
    "    t.transform.translation.z = res_pts[index][2]\n",
    "\n",
    "    #r_quat = tf.transformations.quaternion_from_euler(Roll,Pitch,Yaw)\n",
    "\n",
    "    t.transform.rotation.x = r_quat[0]\n",
    "    t.transform.rotation.y = r_quat[1]\n",
    "    t.transform.rotation.z = r_quat[2]\n",
    "    t.transform.rotation.w = r_quat[3]\n",
    "\n",
    "    br.sendTransform(t)\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533bf35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb115e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8c3bb7",
   "metadata": {},
   "source": [
    "### Use this to fetch transform between two frames that are published in tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e69425b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trans XYZ:                             0.303 0.017 0.456\n",
      "Rot in world coordinate system (deg): 179.947 -1.065 90.001\n"
     ]
    }
   ],
   "source": [
    "transform_plane = fetch_transform(tfbuffer,'world', 'camera_depth_optical_frame',quat=0)\n",
    "\n",
    "transform_plane = np.round(transform_plane,3)\n",
    "print(\"Trans XYZ:                            \",transform_plane[0], transform_plane[1], transform_plane[2])\n",
    "print(\"Rot in world coordinate system (deg):\",transform_plane[3],transform_plane[4],transform_plane[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd1136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503767f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3fb66436",
   "metadata": {},
   "source": [
    "print('Angle: 0')\n",
    "print()\n",
    "print('Positive X count:', np.sum(np.array(nor[:,0]) >= 0, axis=0), \n",
    "      '  Positive Y count:',np.sum(np.array(nor[:,1]) >= 0, axis=0), \n",
    "      '  Positive Z count:',np.sum(np.array(nor[:,2]) >= 0, axis=0), '     Out of:',len(nor[:,0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fd3e3b8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(\"Estimated angle between Normal and Camera\")\n",
    "print()\n",
    "arr = np.array([])\n",
    "for i in range(len(nor)):\n",
    "    arr = np.append(arr, [np.round(np.degrees(angle_between((0,0,1), nor[i])))])\n",
    "    print(i+1,\" \", np.round(np.degrees(angle_between((0,0,1), nor[i]))) )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a712c0ea",
   "metadata": {},
   "source": [
    "arr.mean()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4dd08213",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "hi=np.histogram(arr,bins=10)\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.hist(arr,bins=10)\n",
    "plt.title(\"Angle Distribution of depth Image\")\n",
    "plt.show()\n",
    "unique, counts = np.unique(arr, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff121b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "92ef1e99",
   "metadata": {},
   "source": [
    "header: \n",
    "  seq: 48630\n",
    "  stamp: \n",
    "    secs: 4484\n",
    "    nsecs: 577000000\n",
    "  frame_id: \"camera_depth_optical_frame\"\n",
    "height: 480\n",
    "width: 640\n",
    "distortion_model: \"plumb_bob\"\n",
    "D: []\n",
    "K: [347.99755859375, 0.0, 320.0, 0.0, 347.99755859375, 240.0, 0.0, 0.0, 1.0]\n",
    "R: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "P: [347.99755859375, 0.0, 320.0, 0.0, 0.0, 347.99755859375, 240.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
    "binning_x: 0\n",
    "binning_y: 0\n",
    "roi: \n",
    "  x_offset: 0\n",
    "  y_offset: 0\n",
    "  height: 0\n",
    "  width: 0\n",
    "  do_rectify: False\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a40ae",
   "metadata": {},
   "source": [
    "\n",
    "start_point,end_point = box_pos(320, 240, 10, 10, centered=1)  #x,y,width,height, 0-- top left coord, 1--- center coord\n",
    "\n",
    "cf = color_frame\n",
    "for i in range (color_frame.shape[0]):\n",
    "    for j in range (color_frame.shape[1]):\n",
    "        if depth_frame[i][j] == 0:\n",
    "            cf[i][j] = [0 , 0, 0] \n",
    "\n",
    "plt.imshow(cf)\n",
    "plt.show()\n",
    "window_name = 'Filtered_image'  # Window name in which image is displayed\n",
    "\n",
    "cv2.rectangle(cf, start_point, end_point, (0, 0, 255), 1)\n",
    "cv2.imshow(window_name, cf)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae4c9e",
   "metadata": {},
   "source": [
    "#color_frame_crop = cf[start_point[1]:end_point[1], start_point[0]:end_point[0]]\n",
    "\n",
    "color_frame_crop = cf[start_point[1]:end_point[1], start_point[0]:end_point[0]]\n",
    "depth_frame_crop = depth_frame[start_point[1]:end_point[1], start_point[0]:end_point[0]]\n",
    "\n",
    "print(depth_frame_crop.shape)\n",
    "zz=depth_frame_crop.T\n",
    "print(zz.shape)\n",
    "\n",
    "#cv2.imshow(\"cropped\", depth_frame_crop)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
