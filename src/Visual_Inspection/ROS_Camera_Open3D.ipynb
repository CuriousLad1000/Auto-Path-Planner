{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b930f95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import rospy\n",
    "\n",
    "import pyrealsense2\n",
    "from realsense_depth import *\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import open3d as o3d\n",
    "from scipy import spatial\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import math\n",
    "import tf2_ros, tf\n",
    "import geometry_msgs.msg\n",
    "import PyKDL\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import moveit_commander\n",
    "import moveit_msgs.msg\n",
    "from moveit_commander.conversions import pose_to_list\n",
    "\n",
    "import subprocess\n",
    "import re\n",
    "import copy\n",
    "from PIL import Image as PIL_img\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "image = None\n",
    "\n",
    "rospy.init_node(\"my_pic\", anonymous=True)\n",
    "\n",
    "moveit_commander.roscpp_initialize(sys.argv)\n",
    "robot = moveit_commander.RobotCommander()\n",
    "scene = moveit_commander.PlanningSceneInterface()\n",
    "group_name = \"panda_arm\"\n",
    "move_group = moveit_commander.MoveGroupCommander(group_name) #we'll pass it on while calling functions\n",
    "\n",
    "\n",
    "tfbuffer = tf2_ros.Buffer()\n",
    "listener = tf2_ros.TransformListener(tfbuffer)\n",
    "br = tf2_ros.TransformBroadcaster()\n",
    "t = geometry_msgs.msg.TransformStamped()\n",
    "\n",
    "static_br = tf2_ros.StaticTransformBroadcaster()\n",
    "static_t = geometry_msgs.msg.TransformStamped()\n",
    "\n",
    "bridge = CvBridge()\n",
    "loop_rate = rospy.Rate(0.5) # Node cycle rate (in Hz).\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a31f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!!\n"
     ]
    }
   ],
   "source": [
    "def grab_frame():\n",
    "    \n",
    "    frame_color=rospy.wait_for_message('/camera/color/image_raw', Image, timeout=None) #wait_for_message(topic, topic_type, timeout=None): \n",
    "    cv_image_color = bridge.imgmsg_to_cv2(frame_color, desired_encoding='rgb8')\n",
    "    \n",
    "    frame_depth = rospy.wait_for_message('/camera/depth/image_raw', Image, timeout=None) #wait_for_message(topic, topic_type, timeout=None): \n",
    "    cv_image_depth = bridge.imgmsg_to_cv2(frame_depth)\n",
    "    \n",
    "    return cv_image_color, cv_image_depth\n",
    "\n",
    "def visualize_rgbd(rgbd_image):\n",
    "    print(rgbd_image)\n",
    "    #o3d.visualization.draw_geometries([rgbd_image])\n",
    "    \n",
    "    #intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "    #intrinsic.intrinsic_matrix =  [[462.1379699707031, 0.0, 320.0], [0.0, 462.1379699707031, 240.0], [0.0, 0.0, 1.0]]\n",
    "    #intrinsic.intrinsic_matrix =  [[347.99755859375, 0.0, 320.0], [0.0, 347.99755859375, 240.0], [0.0, 0.0, 1.0]]\n",
    "    #intrinsic.intrinsic_matrix =  [[602.71783447, 0.0, 313.06835938], [0.0, 601.61364746, 230.37461853], [0.0, 0.0, 1.0]]\n",
    "    \n",
    "    #w = 640\n",
    "    #h = 480\n",
    "    #fx = 602.71783447\n",
    "    #fy = 601.61364746\n",
    "    #cx = 313.06835938\n",
    "    #cy = 230.37461853\n",
    "    \n",
    "    #Color frame ROS camera\n",
    "    #w = 640\n",
    "    #h = 480\n",
    "    #fx = 462.1379699707031\n",
    "    #fy = 462.1379699707031\n",
    "    #cx = 320.0\n",
    "    #cy = 240.0\n",
    "    \n",
    "    #Depth frame ROS camera\n",
    "    w = 640\n",
    "    h = 480\n",
    "    fx = 347.99755859375\n",
    "    fy = 347.99755859375\n",
    "    cx = 320.0\n",
    "    cy = 240.0    \n",
    "    \n",
    "    \n",
    "    intrinsic = o3d.camera.PinholeCameraIntrinsic(w, h, fx,fy, cx, cy)\n",
    "    intrinsic.intrinsic_matrix = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]\n",
    "    \n",
    "    cam = o3d.camera.PinholeCameraParameters()\n",
    "    cam.intrinsic = intrinsic\n",
    "    \n",
    "    #cam.extrinsic = np.array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 1.]])\n",
    "    #pcd = o3d.geometry.create_point_cloud_from_rgbd_image(rgbd_image, cam.intrinsic, cam.extrinsic)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, cam.intrinsic)\n",
    "    #pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image,intrinsic)\n",
    "    \n",
    "    pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]) # Flip it, otherwise the pointcloud will be upside down.\n",
    "    \n",
    "    #o3d.visualization.draw_geometries([pcd])\n",
    "    \n",
    "    return pcd\n",
    "    \n",
    "    \n",
    "def tst_dataset(color_frame,depth_frame):\n",
    "    color_raw = o3d.geometry.Image(np.asarray(color_frame))\n",
    "    depth_raw = o3d.geometry.Image(np.asarray(depth_frame.astype(np.uint16)) )\n",
    "    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(color_raw, depth_raw, convert_rgb_to_intensity=False)\n",
    "    pcd = visualize_rgbd(rgbd_image)\n",
    "    return pcd\n",
    "\n",
    "\n",
    "def box_pos(x_coord, y_coord, width, height, centered=0):\n",
    "    if centered == 0:\n",
    "        start_point = (x_coord, y_coord) # represents the top left corner of rectangle\n",
    "        end_point = (x_coord+width-1, y_coord+height-1)  # represents the bottom right corner of rectangle\n",
    "    elif centered == 1:\n",
    "        new_x = x_coord - (np.floor(width/2)-1).astype(int)\n",
    "        new_y = y_coord - (np.floor(height/2)-1).astype(int)\n",
    "        start_point = (new_x, new_y) \n",
    "        end_point = (new_x+width, new_y+height)\n",
    "    return start_point, end_point\n",
    "\n",
    "\n",
    "def fetch_transform(tfbuffer,frame1,frame2,quat=0):\n",
    "    flag = 0\n",
    "    while flag==0:\n",
    "        try:\n",
    "            trans = tfbuffer.lookup_transform(frame1, frame2, rospy.Time(),rospy.Duration(8.0))\n",
    "            #print (trans)\n",
    "            trans = trans.transform  #save translation and rotation\n",
    "            #rot = PyKDL.Rotation.Quaternion(* [ trans.rotation.x,trans.rotation.y,trans.rotation.z,trans.rotation.w] )\n",
    "            #ypr = [ i  / np.pi * 180 for i in rot.GetEulerZYX() ]\n",
    "            #print(ypr[2],ypr[1],ypr[0])\n",
    "            \n",
    "            rot = R.from_quat([ trans.rotation.x,trans.rotation.y,trans.rotation.z,trans.rotation.w]) #creates rotation matrix\n",
    "            rpy = rot.as_euler('XYZ',degrees=True) #extrinsic\n",
    "            break\n",
    "        except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException) as e:\n",
    "            #print (\"Fail\", e)\n",
    "            continue\n",
    "    if quat==0:\n",
    "        return trans.translation.x, trans.translation.y, trans.translation.z, rpy[0],rpy[1],rpy[2] #ypr[2], ypr[1], ypr[0]\n",
    "    elif quat==1:\n",
    "        return trans.translation.x, trans.translation.y, trans.translation.z, trans.rotation.x, trans.rotation.y, trans.rotation.z, trans.rotation.w\n",
    "    \n",
    "\n",
    "\n",
    "def Axis_angle_to_Quat(vector,angle):\n",
    "    x=vector[0]\n",
    "    y=vector[1]\n",
    "    z=vector[2]\n",
    "    qx = x * np.sin(angle/2)\n",
    "    qy = y * np.sin(angle/2)\n",
    "    qz = z * np.sin(angle/2)\n",
    "    qw = np.cos(angle/2)\n",
    "    \n",
    "    return qx,qy,qz,qw\n",
    "\n",
    "def Axis_angle_to_Euler(vector,angle):\n",
    "    x=vector[0]\n",
    "    y=vector[1]\n",
    "    z=vector[2]\n",
    "    s=np.sin(angle)\n",
    "    c=np.cos(angle)\n",
    "    t=1-c\n",
    "    \n",
    "    if ((x*y*t + z*s) > 0.998):  #north pole singularity detected\n",
    "        \n",
    "        heading = 2 * np.arctan2(x*np.sin(angle/2), np.cos(angle/2))\n",
    "        attitude = np.pi/2\n",
    "        bank = 0\n",
    "        return heading, attitude, bank\n",
    "    \n",
    "    elif ((x*y*t + z*s) < -0.998):\n",
    "        \n",
    "        heading = -2 * np.arctan2(x*np.sin(angle/2), np.cos(angle/2))\n",
    "        attitude = -np.pi/2\n",
    "        bank = 0\n",
    "        return heading, attitude, bank\n",
    "    \n",
    "    heading = np.arctan2(y * s- x * z * t , 1 - (y*y+ z*z ) * t)\n",
    "    attitude = np.arcsin(x * y * t + z * s)\n",
    "    bank = np.arctan2(x * s - y * z * t , 1 - (x*x + z*z) * t)\n",
    "    \n",
    "    return bank, heading, attitude\n",
    "\n",
    "\n",
    "def getRotation(v1):\n",
    "    if np.all(v1==[0., 0., 1.]): v1 = [0, 0.000001, 0.999999]\n",
    "    vec_x = [1.,0.,0.] \n",
    "    vec_y = [0.,1.,0.]\n",
    "    vec_z = [0.,0.,1.] #>>>>>>>>>>>>>>>>>>>>> change these to -1 to flip again\n",
    "    \n",
    "    vec1 = v1 / np.linalg.norm(v1)\n",
    "\n",
    "    #vector_x = np.cross(vec1, vec_x)/np.linalg.norm(np.cross(vec1, vec_x))\n",
    "    angle_x = (math.acos(np.dot(vec1, vec_x)))\n",
    "\n",
    "    #vector_y = np.cross(vec1, vec_y)/np.linalg.norm(np.cross(vec1, vec_y))\n",
    "    angle_y = (math.acos(np.dot(vec1, vec_y)))\n",
    "\n",
    "    vector_z = np.cross(vec1, vec_z)/np.linalg.norm(np.cross(vec1, vec_z))\n",
    "    angle_z = -(math.acos(np.dot(vec1, vec_z)))  #<<<Changed to adapt\n",
    "\n",
    "    #Rotation = filtered_Pt_Cloud.get_rotation_matrix_from_axis_angle(angle*vector) #alternative Open3D lib.\n",
    "    Rotation = R.from_rotvec(angle_z*vector_z)\n",
    "    \n",
    "    return Rotation, angle_x, angle_y, angle_z\n",
    "\n",
    "\n",
    "def Bounds_gen(minB, maxB, spacing):\n",
    "    ctr = 0\n",
    "    bounds = np.array([[0.,0.]])\n",
    "    CurrB = minB\n",
    "\n",
    "    while CurrB < maxB-0.5*spacing: #from left most to right most\n",
    "\n",
    "        LowerB = minB + ctr * spacing #if we shift the X or Y coordinates in multiples of spacing, we should get different lines.\n",
    "        if ctr == 0:\n",
    "            CurrB = LowerB + spacing/2 #lower + spacing/2 only for first condition\n",
    "        else:\n",
    "            CurrB = LowerB + spacing #lower + spacing\n",
    "        bounds = np.append(bounds,[[LowerB,CurrB]], axis=0)\n",
    "        #print(\"CurrB:\",CurrB,\"MaxB:\",maxB,\"diff:\",maxB-CurrB)\n",
    "        ctr+= 1\n",
    "    bounds = np.delete(bounds, 0, axis=0)\n",
    "    return bounds\n",
    "\n",
    "def cropped_PC(original_PC, spacing, X=0, idx = 0, centered = True):\n",
    "    PC_BBOX = original_PC.get_axis_aligned_bounding_box()\n",
    "    minB_X = PC_BBOX.min_bound[0]\n",
    "    maxB_X = PC_BBOX.max_bound[0]\n",
    "    minB_Y = PC_BBOX.min_bound[1]\n",
    "    maxB_Y = PC_BBOX.max_bound[1]\n",
    "    minB_Z = PC_BBOX.min_bound[2]\n",
    "    maxB_Z = PC_BBOX.max_bound[2]\n",
    "    nor = np.array(original_PC.normals)\n",
    "    pts = np.array(original_PC.points)\n",
    "\n",
    "    Xs,Ys = get_XY_angles_from_PC(nor)\n",
    "    X_dev = np.std(Xs)\n",
    "    Y_dev = np.std(Ys)\n",
    "\n",
    "    #print(\"X Standard Dev:\", X_dev, \"    Y Standard Dev:\", Y_dev)\n",
    "    if ((X_dev < Y_dev) or (X == 3)): #Manual override at 3\n",
    "        X = 1\n",
    "        print(\"Object's curve around World X-axis:\", X)\n",
    "    elif ((X_dev > Y_dev) or (X == 2)): #Manual override at 2\n",
    "        X = 0\n",
    "        print(\"Object's curve around World Y-axis:\", X)\n",
    "\n",
    "    if X == 1: #X sided sweep crop\n",
    "        bounds = Bounds_gen(minB_X, maxB_X, spacing)\n",
    "        print(\"Total bounds:\",len(bounds))\n",
    "        if centered == True:\n",
    "            idx = int(np.floor(len(bounds)/2))\n",
    "        else:\n",
    "            if idx > len(bounds)-1:\n",
    "                print(\"Warning! Index value out of bounds, setting to max value\")\n",
    "                idx = -1\n",
    "        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(bounds[idx][0], minB_Y, minB_Z), max_bound=(bounds[idx][1], maxB_Y, maxB_Z))\n",
    "\n",
    "        result = original_PC.crop(bbox)\n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "\n",
    "        ind = np.argsort(res_pts[:, 1]) #Sort Y coordinates from lowest to highest, X values are almost constant\n",
    "        res_pts = res_pts[ind]  #no longer required to change values to negative \n",
    "        res_nor = res_nor[ind]\n",
    "\n",
    "    elif X == 0: #Y sided sweep crop\n",
    "        bounds = Bounds_gen(minB_Y, maxB_Y, spacing)\n",
    "        print(\"Total bounds:\",len(bounds))\n",
    "        if centered == True:\n",
    "            idx = int(np.floor(len(bounds)/2))\n",
    "        else:\n",
    "            if idx > len(bounds)-1:\n",
    "                print(\"Warning! Index value out of bounds, setting to max value\")\n",
    "                idx = -1\n",
    "        bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(minB_X, bounds[idx][0], minB_Z), max_bound=(maxB_X, bounds[idx][1], maxB_Z)) \n",
    "        print(bbox)\n",
    "\n",
    "        result = original_PC.crop(bbox)\n",
    "        res_pts = np.array(result.points)\n",
    "        res_nor = np.array(result.normals)\n",
    "\n",
    "        ind = np.argsort(res_pts[:, 0]) #Sort X coordinates from lowest to highest, Y values are almost constant\n",
    "        res_pts = res_pts[ind]\n",
    "        #res_pts[:,2] *=-1  #Change z values to negative\n",
    "        res_nor = res_nor[ind]\n",
    "\n",
    "    sorted_pointcloud = o3d.geometry.PointCloud()\n",
    "    sorted_pointcloud.points = o3d.utility.Vector3dVector(res_pts)\n",
    "    sorted_pointcloud.normals = o3d.utility.Vector3dVector(res_nor)\n",
    "    return sorted_pointcloud\n",
    "\n",
    "\n",
    "def generate_coordinates(point_cloud):\n",
    "    Coordinates = np.array([[0.,0.,0.,0.,0.,0.,0.]])\n",
    "    world_Coordinates = np.array([[0.,0.,0.,0.,0.,0.,0.]])\n",
    "    point_cloud_pts = np.array(point_cloud.points)\n",
    "    point_cloud_nor = np.array(point_cloud.normals)\n",
    "    \n",
    "    for index in range (len(point_cloud_pts)):\n",
    "\n",
    "        rotat, angle_x, angle_y, angle_z = getRotation(-point_cloud_nor[index])  #<<<changed here pts -ve\n",
    "\n",
    "        a1 = rotat.as_matrix()[0][0]\n",
    "        a2 = rotat.as_matrix()[0][1]\n",
    "        a3 = rotat.as_matrix()[0][2]\n",
    "        b1 = rotat.as_matrix()[1][0]\n",
    "        b2 = rotat.as_matrix()[1][1]\n",
    "        b3 = rotat.as_matrix()[1][2]\n",
    "        c1 = rotat.as_matrix()[2][0]\n",
    "        c2 = rotat.as_matrix()[2][1]\n",
    "        c3 = rotat.as_matrix()[2][2]\n",
    "\n",
    "        KDL_original_plane_frame = PyKDL.Frame(PyKDL.Rotation(a1,a2,a3, b1,b2,b3, c1,c2,c3),\n",
    "                                               PyKDL.Vector(point_cloud_pts[index][0],\n",
    "                                                            point_cloud_pts[index][1],\n",
    "                                                             point_cloud_pts[index][2]))\n",
    "\n",
    "        KDL_flip_frame = PyKDL.Frame(PyKDL.Rotation.RPY(0, np.pi, 0), PyKDL.Vector(0,0,0)) #mirror plane frame to match camera frame\n",
    "        KDL_final_frame = KDL_original_plane_frame * KDL_flip_frame\n",
    "        KDL_trans = KDL_final_frame.p\n",
    "        KDL_ROT_quat = KDL_final_frame.M.GetQuaternion() \n",
    "\n",
    "        Coordinates = np.append(Coordinates,[[KDL_trans[0] ,KDL_trans[1],KDL_trans[2] ,KDL_ROT_quat[0] \n",
    "                                     ,KDL_ROT_quat[1] ,KDL_ROT_quat[2],KDL_ROT_quat[3]]],axis=0)\n",
    "\n",
    "        Publish_coordinates([[KDL_trans[0] ,KDL_trans[1],KDL_trans[2] ,KDL_ROT_quat[0] \n",
    "                            ,KDL_ROT_quat[1] ,KDL_ROT_quat[2],KDL_ROT_quat[3]]], \n",
    "                            'camera_depth_optical_frame', 'plane', static = True)\n",
    "\n",
    "        transform_plane = fetch_transform(tfbuffer,'world', 'plane_static_0',quat=1)\n",
    "\n",
    "        world_Coordinates = np.append(world_Coordinates,[[transform_plane[0] ,transform_plane[1]\n",
    "                                            ,transform_plane[2] ,transform_plane[3] ,transform_plane[4] \n",
    "                                             ,transform_plane[5] ,transform_plane[6]]],axis=0)\n",
    "\n",
    "    Coordinates = np.delete(Coordinates, 0, axis=0)\n",
    "    world_Coordinates2 = np.delete(world_Coordinates, 0, axis=0)\n",
    "    return Coordinates, world_Coordinates2\n",
    "\n",
    "def Publish_coordinates(Coordinates, parent_name, child_name, static = False): #Coordinates expects [[0,0,0,0,0,0,0],[1,1,1,1,1,1,1]] format\n",
    "\n",
    "    for index in range (len(Coordinates)):\n",
    "        \n",
    "        if static==True:\n",
    "            static_t.header.stamp = rospy.Time.now()\n",
    "            static_t.header.frame_id = parent_name #\"camera_depth_optical_frame\"\n",
    "            static_t.child_frame_id = child_name+\"_static_\"+str(index)\n",
    "            static_t.transform.translation.x = Coordinates[index][0]\n",
    "            static_t.transform.translation.y = Coordinates[index][1]\n",
    "            static_t.transform.translation.z = Coordinates[index][2]\n",
    "\n",
    "            #r_quat = tf.transformations.quaternion_from_euler(Roll,Pitch,Yaw)\n",
    "            static_t.transform.rotation.x = Coordinates[index][3]\n",
    "            static_t.transform.rotation.y = Coordinates[index][4]\n",
    "            static_t.transform.rotation.z = Coordinates[index][5]\n",
    "            static_t.transform.rotation.w = Coordinates[index][6]\n",
    "            static_br.sendTransform(static_t)\n",
    "        else:\n",
    "            t.header.frame_id = parent_name #\"camera_depth_optical_frame\"\n",
    "            t.child_frame_id = child_name+\"_\"+str(index)\n",
    "\n",
    "            t.header.stamp = rospy.Time.now()\n",
    "            t.transform.translation.x = Coordinates[index][0]\n",
    "            t.transform.translation.y = Coordinates[index][1]\n",
    "            t.transform.translation.z = Coordinates[index][2] \n",
    "\n",
    "            #r_quat = tf.transformations.quaternion_from_euler(Roll,Pitch,Yaw)\n",
    "            t.transform.rotation.x = Coordinates[index][3]\n",
    "            t.transform.rotation.y = Coordinates[index][4]\n",
    "            t.transform.rotation.z = Coordinates[index][5]\n",
    "            t.transform.rotation.w = Coordinates[index][6]\n",
    "            br.sendTransform(t)\n",
    "\n",
    "\n",
    "def Cluster_Point_Cloud(original_PC, eps=0.02, min_points=10):\n",
    "\n",
    "    labels = np.array(original_PC.cluster_dbscan(eps=eps, min_points=min_points))\n",
    "    uniques = np.unique(labels)\n",
    "    clouds = np.array(original_PC)\n",
    "    \n",
    "    if ((uniques[0] == -1) and (len(uniques) == 1)):\n",
    "        clouds = np.append(clouds,[original_PC])\n",
    "\n",
    "    else:\n",
    "        for i in range (len(uniques)):\n",
    "\n",
    "            if uniques[i] > -1:\n",
    "                idx = np.where(labels==uniques[i])[0]\n",
    "                cluster_pcd = original_PC.select_by_index(idx, invert=False)\n",
    "                clouds = np.append(clouds,[cluster_pcd])\n",
    "\n",
    "    clouds = np.delete(clouds, 0)\n",
    "    print(\"Number of clusters:\", len(clouds))\n",
    "    return clouds\n",
    "\n",
    "\n",
    "def go_to_coord_goal(move_group,coord):\n",
    "    pose_goal = geometry_msgs.msg.Pose()\n",
    "    pose_goal.position.x = coord[0]\n",
    "    pose_goal.position.y = coord[1]\n",
    "    pose_goal.position.z = coord[2]\n",
    "    pose_goal.orientation.x = coord[3]\n",
    "    pose_goal.orientation.y = coord[4]\n",
    "    pose_goal.orientation.z = coord[5]\n",
    "    pose_goal.orientation.w = coord[6]\n",
    "\n",
    "    move_group.set_pose_target(pose_goal)\n",
    "    \n",
    "    success = move_group.go(wait=True)\n",
    "    \n",
    "    move_group.stop()\n",
    "    move_group.clear_pose_targets()\n",
    "    current_pose = move_group.get_current_pose().pose\n",
    "\n",
    "def array_to_data(array):\n",
    "    im = PIL_img.fromarray(array)\n",
    "    output_buffer = BytesIO()\n",
    "    im.save(output_buffer, format=\"PNG\")\n",
    "    data = output_buffer.getvalue()\n",
    "    return data\n",
    "\n",
    "def fetch_cloud_image(pointCloud, RX=0, RY=0, RZ=0):\n",
    "    tmp_Rot = pointCloud.get_rotation_matrix_from_xyz((np.radians(RX), np.radians(RY), np.radians((RZ))))\n",
    "    tmp_cloud = copy.deepcopy(pointCloud)  #To avoid overwriting the original point cloud\n",
    "    tmp_cloud.rotate(tmp_Rot, center=(0, 0, 0))\n",
    "    vis = o3d.visualization.Visualizer() \n",
    "    vis.create_window(visible=False, width=640, height=480) \n",
    "    vis.add_geometry(tmp_cloud) \n",
    "    vis.poll_events() \n",
    "    vis.update_renderer() \n",
    "    color = vis.capture_screen_float_buffer(True) \n",
    "    vis.destroy_window() \n",
    "    #color = np.asarray(color)\n",
    "    color = (255.0 * np.asarray(color)).astype(np.uint8)\n",
    "    color = array_to_data(color) #Format according to the GUI requirements\n",
    "    return color\n",
    "\n",
    "def Cluster_selection_gui(clouds):\n",
    "    dat=[]  #to hold cluster's images\n",
    "    pcd_combined = o3d.geometry.PointCloud()\n",
    "    \n",
    "    for current_cloud in clouds:\n",
    "            color = fetch_cloud_image(current_cloud, RX=120)  #different point clouds to be viewed, set view by RX,RY,RZ\n",
    "            dat.append(color)\n",
    "    result = subprocess.run([sys.executable,  \"Cluster_selection_gui.py\"],capture_output=True,text=True,check=True,shell=False, input=repr(dat))\n",
    "    selected_PC = Convert_message(result.stdout)\n",
    "\n",
    "    for k in selected_PC:\n",
    "        pcd_combined += clouds[k]  #combine selected pointclouds \n",
    "    return pcd_combined\n",
    "\n",
    "def Convert_message(string):\n",
    "    return [int(s) for s in re.findall('[0-9]', string)]\n",
    "\n",
    "def get_XY_angles_from_PC(tst_downpcd_nor):\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    vec_x = [1.,0.,0.] \n",
    "    vec_y = [0.,1.,0.]\n",
    "    \n",
    "    for normal_vec in tst_downpcd_nor:\n",
    "        if np.all(normal_vec==[0., 0., 1.]): normal_vec = [0, 0.000001, 0.999999]\n",
    "        vec1 = normal_vec / np.linalg.norm(normal_vec)\n",
    "        angle_x = np.round(np.degrees((math.acos(np.dot(vec1, vec_x)))))\n",
    "        angle_y = np.round(np.degrees((math.acos(np.dot(vec1, vec_y)))))\n",
    "        Xs.append(angle_x)\n",
    "        Ys.append(angle_y)\n",
    "    Xs = np.array(Xs)\n",
    "    Ys = np.array(Ys)\n",
    "    return Xs, Ys\n",
    "\n",
    "\n",
    "print(\"Loaded!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6be761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to initial position\n"
     ]
    }
   ],
   "source": [
    "#move_group.get_current_pose()\n",
    "initial_coordinates = [0.408,0.,0.834,0.9238709648606045,-0.3826807475252162,-0.003837162229149499,0.001778186465088758]\n",
    "move_group.get_current_pose()\n",
    "\n",
    "go_to_coord_goal(move_group, initial_coordinates) #pass values to function to make robot move in cartesian space. \n",
    "print(\"Moving to initial position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d302cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color frame: (480, 640, 3)  Depth frame: (480, 640)\n",
      "Center of image: 831 mm\n",
      "RGBDImage of size \n",
      "Color image : 640x480, with 3 channels.\n",
      "Depth image : 640x480, with 1 channels.\n",
      "Use numpy.asarray to access buffer data.\n",
      "Recompute the normal of the downsampled point cloud\n",
      "Align normals towards camera\n",
      "AxisAlignedBoundingBox: min: (-0.35456, -0.0621719, -0.7898), max: (0.0950811, 0.542872, -0.605)\n",
      "PointCloud with 468 points.\n",
      "\n",
      "0.021067636297005355\n",
      "146\n",
      "\n",
      "Center Coordinates(ground truth): [-0.24962445 -0.01448434  0.63045913]\n",
      "Points Coordinates(estimated center point): [-0.24143776 -0.01525523  0.6110625 ]\n",
      "Normal Coordinates(Normal of estimated center point): [ 0.32028753 -0.0334854  -0.94672838]\n"
     ]
    }
   ],
   "source": [
    "color_frame, depth_frame = grab_frame()\n",
    "print(\"color frame:\",color_frame.shape, \" Depth frame:\",depth_frame.shape)\n",
    "\n",
    "#Check center pixel distance\n",
    "\n",
    "point = (320, 240)\n",
    "\n",
    "spacing = 0.01 #spacing between each point in point cloud in meters (use this to approximate the shape of surface)\n",
    "offset_y = 0.13 #distance to crop in world's X axis away from robot, to avoid robot's shadow appearing in PC.\n",
    "offset_z = 0#0.05\n",
    "mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1,origin=[0, 0, 0])\n",
    "\n",
    "distance = depth_frame[point[1], point[0]]\n",
    "print(\"Center of image:\", distance, \"mm\")\n",
    "\n",
    "pt_cloud = tst_dataset(color_frame, depth_frame)\n",
    "\n",
    "downpcd = pt_cloud.voxel_down_sample(voxel_size=spacing)\n",
    "\n",
    "print(\"Recompute the normal of the downsampled point cloud\")\n",
    "downpcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.06, max_nn=30)) #radius in meters\n",
    "print(\"Align normals towards camera\")\n",
    "downpcd.orient_normals_towards_camera_location(camera_location=[0, 0, 0])\n",
    "\n",
    "#o3d.visualization.draw_geometries([downpcd,mesh], point_show_normal=True)\n",
    "\n",
    "## Filter out hidden points\n",
    "diameter = np.linalg.norm(np.asarray(downpcd.get_max_bound()) - np.asarray(downpcd.get_min_bound()))\n",
    "cam = [0, 0, diameter]\n",
    "radius = diameter * 100\n",
    "_, pt_map = downpcd.hidden_point_removal(cam, radius) #Get all points that are visible from given view point\n",
    "downpcd = downpcd.select_by_index(pt_map)\n",
    "#o3d.visualization.draw_geometries([downpcd,mesh], point_show_normal=True)\n",
    "\n",
    "\n",
    "#v2_camera = np.array(downpcd.normals)\n",
    "v2_camera_pts = np.array(downpcd.points)\n",
    "\n",
    "\n",
    "#idx = np.where(abs(v2_camera_pts[:,2]) < (distance/1000)+0.1)[0] #fetch all indexes of values less than distance of center of img 0.3 in column 3 of rec\n",
    "idx = np.where(abs(v2_camera_pts[:,2]) < abs(np.min(v2_camera_pts[:,2]))-0.06)[0] #fetch all indexes of values less than distance of center of img 0.3 in column 3 of rec\n",
    "\n",
    "#print(idx)\n",
    "filtered_Pt_Cloud = downpcd.select_by_index(idx, invert=False)\n",
    "\n",
    "\n",
    "#CROP TO FILTER OUT ROBOT'S SHADOW ADJUST OFFSET ACCORDINGLY\n",
    "PC_BBOX = filtered_Pt_Cloud.get_axis_aligned_bounding_box()\n",
    "print(PC_BBOX)\n",
    "minB_X = PC_BBOX.min_bound[0]\n",
    "maxB_X = PC_BBOX.max_bound[0]\n",
    "minB_Y = PC_BBOX.min_bound[1]\n",
    "maxB_Y = PC_BBOX.max_bound[1]\n",
    "minB_Z = PC_BBOX.min_bound[2]\n",
    "maxB_Z = PC_BBOX.max_bound[2]\n",
    "bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound=(minB_X, minB_Y, minB_Z+offset_z), max_bound=(maxB_X, maxB_Y-offset_y, maxB_Z)) \n",
    "filtered_Pt_Cloud = filtered_Pt_Cloud.crop(bbox)\n",
    "\n",
    "Re = filtered_Pt_Cloud.get_rotation_matrix_from_xyz((0, np.pi, np.pi))\n",
    "filtered_Pt_Cloud = filtered_Pt_Cloud.rotate(Re, center=(0,0,0))\n",
    "\n",
    "print(filtered_Pt_Cloud)\n",
    "o3d.visualization.draw_geometries([filtered_Pt_Cloud,mesh], point_show_normal=True)\n",
    "\n",
    "\n",
    "\n",
    "nor = np.array(filtered_Pt_Cloud.normals)\n",
    "pts = np.array(filtered_Pt_Cloud.points)\n",
    "\n",
    "print()\n",
    "distance,index = spatial.KDTree(pts).query( filtered_Pt_Cloud.get_center() ) #find coordinates \n",
    "                                                                             #that are closest to the center \n",
    "\n",
    "print(distance)\n",
    "print(index)\n",
    "print()\n",
    "print(\"Center Coordinates(ground truth):\",filtered_Pt_Cloud.get_center())\n",
    "print(\"Points Coordinates(estimated center point):\",pts[index])\n",
    "print(\"Normal Coordinates(Normal of estimated center point):\",nor[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b51d7",
   "metadata": {},
   "source": [
    "### Clustering of points in Point cloud\n",
    "ref: http://www.open3d.org/docs/release/tutorial/geometry/pointcloud.html#DBSCAN-clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e32e8",
   "metadata": {},
   "source": [
    "### Crop point cloud to get a points in a single line in X or Y axis, \n",
    "*Use idx to give offset else use centered to get the points from center of the object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d343540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 1\n",
      "Object's curve around World Y-axis: 0\n",
      "Total bounds: 16\n",
      "AxisAlignedBoundingBox: min: (-0.35456, -0.0171386, 0.605), max: (-0.175345, -0.00713861, 0.6925)\n"
     ]
    }
   ],
   "source": [
    "clouds = Cluster_Point_Cloud(filtered_Pt_Cloud, eps=0.05, min_points=10)\n",
    "\n",
    "result = Cluster_selection_gui(clouds) #Will present a GUI to select all relevant pointclouds\n",
    "#o3d.visualization.draw_geometries([result, mesh], point_show_normal=True)\n",
    "\n",
    "result = cropped_PC(result, spacing, X=0, idx = 0, centered = True)\n",
    "o3d.visualization.draw_geometries([result, mesh], point_show_normal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf797ad",
   "metadata": {},
   "source": [
    "### This will generate and boradcast all points in pointcloud to tf2 or tf2_static topic, visualize using Rviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c84aed30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total coordinates: 27\n"
     ]
    }
   ],
   "source": [
    "cam_coords, world_coords = generate_coordinates(result)\n",
    "#cam_coords, world_coords = generate_coordinates(clouds[0])\n",
    "print(\"Total coordinates:\",len(world_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ddeb20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Publish_coordinates(coords,'camera_depth_optical_frame','plane' static = False)\n",
    "for i in range (len(world_coords)):\n",
    "    Publish_coordinates([world_coords[i]],'world','plane', static = True)\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f63a8",
   "metadata": {},
   "source": [
    "## Robot Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db440c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"============ Printing robot current Joint values\") #O/P: current joint values of the end-effector\n",
    "#print(move_group.get_current_joint_values())\n",
    "print(\"In Degrees: \", np.degrees( move_group.get_current_joint_values() ) ) #O/P joint angles in degrees\n",
    "print(\"\")\n",
    "\n",
    "print(\"============ Printing robot current pose\") #O/P will be XYZ and xyz w  positions and orientations of the robot.\n",
    "print(move_group.get_current_pose())\n",
    "print(\"\")\n",
    "\n",
    "print(\"============ Printing robot current RPY\") #O/P wil be orientation of end-effector in RPY (Radians)\n",
    "#print(move_group.get_current_rpy())\n",
    "print(\"RPY In Degrees: \", np.degrees( move_group.get_current_rpy() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb115e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47400390238291623, -0.5685850632147568, 0.41742419242227324, 0.6642908791909976, 0.6602634842326803, 0.24323442646096985, -0.2522038322404617]\n",
      "Moving to Target\n",
      "[0.4710532915674389, -0.5241051886906012, 0.4549547607511857, 0.6759744281490649, 0.6724926511146448, 0.213399206949274, -0.21277449367526044]\n",
      "Moving to Target\n",
      "[0.47436910778238905, -0.39928782396635565, 0.5128575512264575, 0.7018619911206407, 0.7002118233801191, 0.08875343924792962, -0.09599986896096503]\n",
      "Moving to Target\n",
      "[0.4764726886795963, -0.2917916137561692, 0.5271884361915091, 0.7071103596619345, 0.7070123961913422, -0.008619275715343083, 0.0073565601125138]\n",
      "Moving to Target\n",
      "[0.46805232742163055, -0.2253801914552636, 0.5196742662891525, 0.7034242287947797, 0.704262357052743, -0.06368274441833602, 0.07178714959052987]\n",
      "Moving to Target\n",
      "[0.47428643412642624, -0.10987456931000253, 0.48120077012684564, 0.6837916291230245, 0.6862644910853526, -0.17903879297176406, 0.17150850366047707]\n",
      "Moving to Target\n",
      "[0.47097254504830466, -0.0641224878488717, 0.4529943815279751, 0.6729325123660548, 0.6759936810662831, -0.2119309977848747, 0.21279010582513866]\n",
      "Moving to Target\n",
      "[0.46674714521663374, -0.005068512201317765, 0.4018208315352952, 0.6488076373352697, 0.6528771685089134, -0.2711504107975994, 0.28156261701352037]\n",
      "Moving to Target\n",
      "[0.4636612424604599, 0.06285911238493594, 0.3045794393910809, 0.5990386575209955, 0.6045917255940353, -0.36300616856464124, 0.37927305957936197]\n",
      "Moving to Target\n"
     ]
    }
   ],
   "source": [
    "z_offset = 0.3\n",
    "\n",
    "Publish_coordinates([world_coords[0]],'world','plane', static = True)\n",
    "time.sleep(3)\n",
    "\n",
    "for id_x in range(0,len(world_coords),3):  #Take every 4th point in coordinates\n",
    "    transform_world_plane = world_coords[id_x]\n",
    "    KDL_original_plane_frame = PyKDL.Frame(PyKDL.Rotation.Quaternion(transform_world_plane[3],\n",
    "                                                                     transform_world_plane[4], \n",
    "                                                                     transform_world_plane[5], \n",
    "                                                                     transform_world_plane[6]),\n",
    "                                           PyKDL.Vector(transform_world_plane[0], \n",
    "                                                        transform_world_plane[1], \n",
    "                                                        transform_world_plane[2]))\n",
    "\n",
    "    trans_x,trans_y,trans_z = KDL_original_plane_frame * PyKDL.Vector(0, 0, z_offset) #Add offset\n",
    "    KDL_original_plane_frame.p = PyKDL.Vector(trans_x,trans_y,trans_z) #update original plane frame to new location\n",
    "\n",
    "    KDL_flip_frame = PyKDL.Frame(PyKDL.Rotation.RPY(np.pi, 0.,np.pi), PyKDL.Vector(0, 0, 0)) #mirror plane frame to match camera frame\n",
    "\n",
    "    KDL_final_frame = KDL_original_plane_frame * KDL_flip_frame\n",
    "\n",
    "    KDL_trans = KDL_final_frame.p\n",
    "    KDL_ROT_quat = KDL_final_frame.M.GetQuaternion() \n",
    "\n",
    "    final_coordinates = [KDL_trans[0], KDL_trans[1], KDL_trans[2], KDL_ROT_quat[0], KDL_ROT_quat[1], \n",
    "                                                                   KDL_ROT_quat[2], KDL_ROT_quat[3]]\n",
    "\n",
    "    print(final_coordinates)\n",
    "\n",
    "    Publish_coordinates([final_coordinates], \"world\", 'Camera_Target', static = True)\n",
    "    \n",
    "    #Fetch transform between link8 and optical depth cam\n",
    "    transform_link8_camera_depth = fetch_transform(tfbuffer,'camera_depth_optical_frame', 'panda_link8',quat=1)\n",
    "\n",
    "    KDL_link8_cam_frame = PyKDL.Frame(PyKDL.Rotation.Quaternion(transform_link8_camera_depth[3],\n",
    "                                                                transform_link8_camera_depth[4], \n",
    "                                                                transform_link8_camera_depth[5],\n",
    "                                                                transform_link8_camera_depth[6]),\n",
    "                                      PyKDL.Vector(transform_link8_camera_depth[0], \n",
    "                                                   transform_link8_camera_depth[1], \n",
    "                                                   transform_link8_camera_depth[2])) #link8-camera frame\n",
    "\n",
    "    #Offset original frame by z offset, flip the frame to match camera, multiply with link8-camera frame to replicate\n",
    "    #pose between camera and link8\n",
    "    KDL_final_frame = KDL_original_plane_frame * KDL_flip_frame * KDL_link8_cam_frame \n",
    "\n",
    "    KDL_trans = KDL_final_frame.p\n",
    "    KDL_ROT_quat = KDL_final_frame.M.GetQuaternion() \n",
    "\n",
    "    final_coordinates = [KDL_trans[0], KDL_trans[1], KDL_trans[2], KDL_ROT_quat[0], KDL_ROT_quat[1], \n",
    "                                                                   KDL_ROT_quat[2], KDL_ROT_quat[3]]\n",
    "    Publish_coordinates([final_coordinates], \"world\", 'EEF_Target', static = True)\n",
    "\n",
    "    ## Move Robot to TARGET\n",
    "    go_to_coord_goal(move_group, final_coordinates) #pass values to function to make robot move in cartesian space. \n",
    "    print(\"Moving to Target\")\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c3bb7",
   "metadata": {},
   "source": [
    "### Use this to fetch transform between two frames that are published in tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_transform(tfbuffer,'panda_hand', 'panda_link8',quat=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_transform(tfbuffer,'panda_hand', 'camera_bottom_screw_frame',quat=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc301c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_transform(tfbuffer,'panda_link8', 'camera_bottom_screw_frame',quat=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede69437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3fb66436",
   "metadata": {},
   "source": [
    "print('Angle: 0')\n",
    "print()\n",
    "print('Positive X count:', np.sum(np.array(nor[:,0]) >= 0, axis=0), \n",
    "      '  Positive Y count:',np.sum(np.array(nor[:,1]) >= 0, axis=0), \n",
    "      '  Positive Z count:',np.sum(np.array(nor[:,2]) >= 0, axis=0), '     Out of:',len(nor[:,0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fd3e3b8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(\"Estimated angle between Normal and Camera\")\n",
    "print()\n",
    "arr = np.array([])\n",
    "for i in range(len(nor)):\n",
    "    arr = np.append(arr, [np.round(np.degrees(angle_between((0,0,1), nor[i])))])\n",
    "    print(i+1,\" \", np.round(np.degrees(angle_between((0,0,1), nor[i]))) )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a712c0ea",
   "metadata": {},
   "source": [
    "arr.mean()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4dd08213",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "hi=np.histogram(arr,bins=10)\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.hist(arr,bins=10)\n",
    "plt.title(\"Angle Distribution of depth Image\")\n",
    "plt.show()\n",
    "unique, counts = np.unique(arr, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff121b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "92ef1e99",
   "metadata": {},
   "source": [
    "header: \n",
    "  seq: 48630\n",
    "  stamp: \n",
    "    secs: 4484\n",
    "    nsecs: 577000000\n",
    "  frame_id: \"camera_depth_optical_frame\"\n",
    "height: 480\n",
    "width: 640\n",
    "distortion_model: \"plumb_bob\"\n",
    "D: []\n",
    "K: [347.99755859375, 0.0, 320.0, 0.0, 347.99755859375, 240.0, 0.0, 0.0, 1.0]\n",
    "R: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "P: [347.99755859375, 0.0, 320.0, 0.0, 0.0, 347.99755859375, 240.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
    "binning_x: 0\n",
    "binning_y: 0\n",
    "roi: \n",
    "  x_offset: 0\n",
    "  y_offset: 0\n",
    "  height: 0\n",
    "  width: 0\n",
    "  do_rectify: False\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a40ae",
   "metadata": {},
   "source": [
    "\n",
    "start_point,end_point = box_pos(320, 240, 10, 10, centered=1)  #x,y,width,height, 0-- top left coord, 1--- center coord\n",
    "\n",
    "cf = color_frame\n",
    "for i in range (color_frame.shape[0]):\n",
    "    for j in range (color_frame.shape[1]):\n",
    "        if depth_frame[i][j] == 0:\n",
    "            cf[i][j] = [0 , 0, 0] \n",
    "\n",
    "plt.imshow(cf)\n",
    "plt.show()\n",
    "window_name = 'Filtered_image'  # Window name in which image is displayed\n",
    "\n",
    "cv2.rectangle(cf, start_point, end_point, (0, 0, 255), 1)\n",
    "cv2.imshow(window_name, cf)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae4c9e",
   "metadata": {},
   "source": [
    "#color_frame_crop = cf[start_point[1]:end_point[1], start_point[0]:end_point[0]]\n",
    "\n",
    "color_frame_crop = cf[start_point[1]:end_point[1], start_point[0]:end_point[0]]\n",
    "depth_frame_crop = depth_frame[start_point[1]:end_point[1], start_point[0]:end_point[0]]\n",
    "\n",
    "print(depth_frame_crop.shape)\n",
    "zz=depth_frame_crop.T\n",
    "print(zz.shape)\n",
    "\n",
    "#cv2.imshow(\"cropped\", depth_frame_crop)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
